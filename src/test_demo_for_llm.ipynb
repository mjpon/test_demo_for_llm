{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-06-29T18:11:36.756053Z",
     "start_time": "2023-06-29T18:11:35.111819Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mitchell.pon%40roche.com:****@navify.jfrog.io/navify/api/pypi/navify-pypi/simple, https://pypi.org/simple\r\n",
      "Requirement already satisfied: llama-index in /Users/ponm/Documents/Personal Projects/test_demo_for_llm/venv/lib/python3.9/site-packages (0.6.35)\r\n",
      "Requirement already satisfied: urllib3<2 in /Users/ponm/Documents/Personal Projects/test_demo_for_llm/venv/lib/python3.9/site-packages (from llama-index) (1.26.16)\r\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /Users/ponm/Documents/Personal Projects/test_demo_for_llm/venv/lib/python3.9/site-packages (from llama-index) (8.2.2)\r\n",
      "Requirement already satisfied: numpy in /Users/ponm/Documents/Personal Projects/test_demo_for_llm/venv/lib/python3.9/site-packages (from llama-index) (1.25.0)\r\n",
      "Requirement already satisfied: tiktoken in /Users/ponm/Documents/Personal Projects/test_demo_for_llm/venv/lib/python3.9/site-packages (from llama-index) (0.4.0)\r\n",
      "Requirement already satisfied: pandas in /Users/ponm/Documents/Personal Projects/test_demo_for_llm/venv/lib/python3.9/site-packages (from llama-index) (2.0.3)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/ponm/Documents/Personal Projects/test_demo_for_llm/venv/lib/python3.9/site-packages (from llama-index) (2023.6.0)\r\n",
      "Requirement already satisfied: typing-extensions==4.5.0 in /Users/ponm/Documents/Personal Projects/test_demo_for_llm/venv/lib/python3.9/site-packages (from llama-index) (4.5.0)\r\n",
      "Requirement already satisfied: dataclasses-json in /Users/ponm/Documents/Personal Projects/test_demo_for_llm/venv/lib/python3.9/site-packages (from llama-index) (0.5.8)\r\n",
      "Requirement already satisfied: sqlalchemy>=2.0.15 in /Users/ponm/Documents/Personal Projects/test_demo_for_llm/venv/lib/python3.9/site-packages (from llama-index) (2.0.17)\r\n",
      "Requirement already satisfied: typing-inspect==0.8.0 in /Users/ponm/Documents/Personal Projects/test_demo_for_llm/venv/lib/python3.9/site-packages (from llama-index) (0.8.0)\r\n",
      "Requirement already satisfied: langchain>=0.0.154 in /Users/ponm/Documents/Personal Projects/test_demo_for_llm/venv/lib/python3.9/site-packages (from llama-index) (0.0.219)\r\n",
      "Requirement already satisfied: openai>=0.26.4 in /Users/ponm/Documents/Personal Projects/test_demo_for_llm/venv/lib/python3.9/site-packages (from llama-index) (0.27.8)\r\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/ponm/Documents/Personal Projects/test_demo_for_llm/venv/lib/python3.9/site-packages (from llama-index) (4.12.2)\r\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/ponm/Documents/Personal Projects/test_demo_for_llm/venv/lib/python3.9/site-packages (from typing-inspect==0.8.0->llama-index) (1.0.0)\r\n",
      "Requirement already satisfied: langchainplus-sdk>=0.0.17 in /Users/ponm/Documents/Personal Projects/test_demo_for_llm/venv/lib/python3.9/site-packages (from langchain>=0.0.154->llama-index) (0.0.17)\r\n",
      "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in /Users/ponm/Documents/Personal Projects/test_demo_for_llm/venv/lib/python3.9/site-packages (from langchain>=0.0.154->llama-index) (1.2.4)\r\n",
      "Requirement already satisfied: pydantic<2,>=1 in /Users/ponm/Documents/Personal Projects/test_demo_for_llm/venv/lib/python3.9/site-packages (from langchain>=0.0.154->llama-index) (1.10.9)\r\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/ponm/Documents/Personal Projects/test_demo_for_llm/venv/lib/python3.9/site-packages (from langchain>=0.0.154->llama-index) (2.31.0)\r\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /Users/ponm/Documents/Personal Projects/test_demo_for_llm/venv/lib/python3.9/site-packages (from langchain>=0.0.154->llama-index) (4.0.2)\r\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in /Users/ponm/Documents/Personal Projects/test_demo_for_llm/venv/lib/python3.9/site-packages (from langchain>=0.0.154->llama-index) (6.0)\r\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/ponm/Documents/Personal Projects/test_demo_for_llm/venv/lib/python3.9/site-packages (from langchain>=0.0.154->llama-index) (3.8.4)\r\n",
      "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /Users/ponm/Documents/Personal Projects/test_demo_for_llm/venv/lib/python3.9/site-packages (from langchain>=0.0.154->llama-index) (2.8.4)\r\n",
      "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /Users/ponm/Documents/Personal Projects/test_demo_for_llm/venv/lib/python3.9/site-packages (from dataclasses-json->llama-index) (1.5.1)\r\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /Users/ponm/Documents/Personal Projects/test_demo_for_llm/venv/lib/python3.9/site-packages (from dataclasses-json->llama-index) (3.19.0)\r\n",
      "Requirement already satisfied: tqdm in /Users/ponm/Documents/Personal Projects/test_demo_for_llm/venv/lib/python3.9/site-packages (from openai>=0.26.4->llama-index) (4.65.0)\r\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/ponm/Documents/Personal Projects/test_demo_for_llm/venv/lib/python3.9/site-packages (from beautifulsoup4->llama-index) (2.4.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/ponm/Documents/Personal Projects/test_demo_for_llm/venv/lib/python3.9/site-packages (from pandas->llama-index) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/ponm/Documents/Personal Projects/test_demo_for_llm/venv/lib/python3.9/site-packages (from pandas->llama-index) (2023.3)\r\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/ponm/Documents/Personal Projects/test_demo_for_llm/venv/lib/python3.9/site-packages (from pandas->llama-index) (2023.3)\r\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/ponm/Documents/Personal Projects/test_demo_for_llm/venv/lib/python3.9/site-packages (from tiktoken->llama-index) (2023.6.3)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/ponm/Documents/Personal Projects/test_demo_for_llm/venv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.154->llama-index) (1.3.3)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/ponm/Documents/Personal Projects/test_demo_for_llm/venv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.154->llama-index) (23.1.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/ponm/Documents/Personal Projects/test_demo_for_llm/venv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.154->llama-index) (3.1.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/ponm/Documents/Personal Projects/test_demo_for_llm/venv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.154->llama-index) (6.0.4)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/ponm/Documents/Personal Projects/test_demo_for_llm/venv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.154->llama-index) (1.9.2)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/ponm/Documents/Personal Projects/test_demo_for_llm/venv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.154->llama-index) (1.3.1)\r\n",
      "Requirement already satisfied: packaging>=17.0 in /Users/ponm/Documents/Personal Projects/test_demo_for_llm/venv/lib/python3.9/site-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json->llama-index) (23.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/ponm/Documents/Personal Projects/test_demo_for_llm/venv/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->llama-index) (1.16.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ponm/Documents/Personal Projects/test_demo_for_llm/venv/lib/python3.9/site-packages (from requests<3,>=2->langchain>=0.0.154->llama-index) (2023.5.7)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ponm/Documents/Personal Projects/test_demo_for_llm/venv/lib/python3.9/site-packages (from requests<3,>=2->langchain>=0.0.154->llama-index) (3.4)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip available: \u001B[0m\u001B[31;49m22.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.1.2\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Looking in indexes: https://mitchell.pon%40roche.com:****@navify.jfrog.io/navify/api/pypi/navify-pypi/simple, https://pypi.org/simple\r\n",
      "Requirement already satisfied: openai in /Users/ponm/Documents/Personal Projects/test_demo_for_llm/venv/lib/python3.9/site-packages (0.27.8)\r\n",
      "Requirement already satisfied: requests>=2.20 in /Users/ponm/Documents/Personal Projects/test_demo_for_llm/venv/lib/python3.9/site-packages (from openai) (2.31.0)\r\n",
      "Requirement already satisfied: aiohttp in /Users/ponm/Documents/Personal Projects/test_demo_for_llm/venv/lib/python3.9/site-packages (from openai) (3.8.4)\r\n",
      "Requirement already satisfied: tqdm in /Users/ponm/Documents/Personal Projects/test_demo_for_llm/venv/lib/python3.9/site-packages (from openai) (4.65.0)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ponm/Documents/Personal Projects/test_demo_for_llm/venv/lib/python3.9/site-packages (from requests>=2.20->openai) (1.26.16)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ponm/Documents/Personal Projects/test_demo_for_llm/venv/lib/python3.9/site-packages (from requests>=2.20->openai) (2023.5.7)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ponm/Documents/Personal Projects/test_demo_for_llm/venv/lib/python3.9/site-packages (from requests>=2.20->openai) (3.4)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ponm/Documents/Personal Projects/test_demo_for_llm/venv/lib/python3.9/site-packages (from requests>=2.20->openai) (3.1.0)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/ponm/Documents/Personal Projects/test_demo_for_llm/venv/lib/python3.9/site-packages (from aiohttp->openai) (4.0.2)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/ponm/Documents/Personal Projects/test_demo_for_llm/venv/lib/python3.9/site-packages (from aiohttp->openai) (1.3.3)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/ponm/Documents/Personal Projects/test_demo_for_llm/venv/lib/python3.9/site-packages (from aiohttp->openai) (1.9.2)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/ponm/Documents/Personal Projects/test_demo_for_llm/venv/lib/python3.9/site-packages (from aiohttp->openai) (6.0.4)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/ponm/Documents/Personal Projects/test_demo_for_llm/venv/lib/python3.9/site-packages (from aiohttp->openai) (23.1.0)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/ponm/Documents/Personal Projects/test_demo_for_llm/venv/lib/python3.9/site-packages (from aiohttp->openai) (1.3.1)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip available: \u001B[0m\u001B[31;49m22.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.1.2\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install llama-index\n",
    "!pip install openai\n",
    "!pip install nest_asyncio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "[Document(id_='981c95f1-be04-4060-b684-c2925bb51bd3', embedding=None, metadata={'page_label': '1', 'file_name': 'amazon-dynamo-sosp2007.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='bbddf19043b3c06fe7841a74502fbc6dea6c7729f7973ed6cbec6e7c3ef2edc4', text='Dynamo: Amazon’s Highly Available Key-value Store \\nGiuseppe DeCandia, Deniz Ha storun, Madan Jampani, Guna vardhan Kakulapati,  \\nAvinash Lakshman, Alex Pilchin, Swam inathan Sivasubramanian, Peter Vosshall  \\nand Werner Vogels \\nAmazon.com \\n \\nABSTRACT \\nReliability at massive scale is one of the biggest challenges we \\nface at Amazon.com, one of the largest e-commerce operations in \\nthe world; even the slightest outage has significant financial \\nconsequences and impacts cust omer trust. The Amazon.com \\nplatform, which provides services for many web sites worldwide, \\nis implemented on top of an infrastr ucture of tens of thousands of \\nservers and network components located in many datacenters \\naround the world. At this scale, small and large components fail \\ncontinuously and the way persistent  state is managed in the face \\nof these failures drives the reliability and scalability of the \\nsoftware systems.  \\nThis paper presents the design and implementation of Dynamo, a \\nhighly available key-value storage system that some of Amazon’s \\ncore services use to provide an “always-on” experience.  To \\nachieve this level of availability , Dynamo sacrifices consistency \\nunder certain failure scenarios. It  makes extensive use of object \\nversioning and application-assisted  conflict resolution in a manner \\nthat provides a novel interfac e for developers to use. \\nCategories and Subject Descriptors  \\nD.4.2 [ Operating Systems ]: Storage Management; D.4.5 \\n[Operating Systems ]: Reliability; D.4.2 [ Operating Systems ]: \\nPerformance;  \\nGeneral Terms  \\nAlgorithms, Management, Measur ement, Performance, Design, \\nReliability. \\n1. INTRODUCTION  \\nAmazon runs a world-wide e-commerce platform that serves tens \\nof millions customers at peak times using tens of thousands of \\nservers located in many data centers around the world. There are \\nstrict operational requirements on Amazon’s platform in terms of \\nperformance, reliabilit y and efficiency, and to support continuous \\ngrowth the platform needs to be highly scalable. Reliability is one \\nof the most important requirements because even the slightest \\noutage has significant financia l consequences and impacts \\ncustomer trust. In addition, to  support continuous growth, the \\nplatform needs to be highly scalable. One of the lessons our organiza tion has learned from operating \\nAmazon’s platform is that the reliability and scalability of a \\nsystem is dependent on how its application stat e is managed. \\nAmazon uses a highly decentralized, loosely coupled, service oriented architecture consisting of hundreds of services. In this \\nenvironment there is a particular need for storage technologies \\nthat are always available. For ex ample, customers should be able \\nto view and add items to their shopping cart even if disks are \\nfailing, network routes are flapping, or data centers are being \\ndestroyed by tornados. Therefore, the service responsible for \\nmanaging shopping carts requires that it can always write to and \\nread from its data store, and that its data needs to be available \\nacross multiple data centers.  \\nDealing with failures in an infra structure comprised of millions of \\ncomponents is our standard mode of operation; there are always a \\nsmall but significant number of server and network components \\nthat are failing at any given time. As such Amazon’s software \\nsystems need to be constructed in a manner that treats failure \\nhandling as the normal case wit hout impacting availability or \\nperformance. \\nTo meet the reliability and sca ling needs, Amazon has developed \\na number of storage technologies, of which the Amazon Simple \\nStorage Service (also available outside of Amazon and known as \\nAmazon S3), is probably the best known. This paper presents the \\ndesign and implementation of Dy namo, another highly available \\nand scalable distributed data store built for Amazon’s platform. \\nDynamo is used to manage the state of services that have very \\nhigh reliability requirements a nd need tight control over the \\ntradeoffs between availability, consistency, cost-effectiveness and \\nperformance. Amazon’s platform has a very diverse set of \\napplications with different storag e requirements. A select set of \\napplications requires a storage t echnology that is flexible enough \\nto let application designers confi gure their data store appropriately \\nbased on these tradeoffs to achieve high availability and \\nguaranteed performance in the most cost effective manner. \\nThere are many services on Amazon’s platform that only need \\nprimary-key access to a data store. For many services, such as \\nthose that provide best seller  lists, shopping carts, customer \\npreferences, session management, sa les rank, and product catalog, \\nthe common pattern of using a rela tional database would lead to \\ninefficiencies and limit scale and availability. Dynamo provides a \\nsimple primary-key only interface to meet the requirements of \\nthese applications.  \\nDynamo uses a synthesis of well known techniques to achieve \\nscalability and availability: Data is partitioned and replicated \\nusing consistent hashing [10], a nd consistency is facilitated by \\nobject versioning [12]. The cons istency among replicas during \\nupdates is maintained by a quorum-like technique and a \\ndecentralized replica synchronization protocol. Dynamo employs  \\n \\nPermission to make digital or hard copi es of all or part of this work for \\npersonal or classroom use is granted without fee provided that copies are \\nnot made or distributed for profit or commercial advantage and that \\ncopies bear this notice and the full citation on the first page. To copy otherwise, or republish, to post on servers or to redistribute to lists, \\nrequires prior specific permission and/or a fee. \\nSOSP’07,  October 14–17, 2007, Stevenson, Washington, USA. \\nCopyright 2007 ACM 978-1-59593- 591-5/07/0010...$5.00. \\n195 \\n205 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n Document(id_='6a636f02-f324-411d-bf3d-51ae68484a75', embedding=None, metadata={'page_label': '2', 'file_name': 'amazon-dynamo-sosp2007.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='13da229ea363c02d6c6117b8a192718781c778713bbc98cbb226522e382a3bed', text='a gossip based distributed fail ure detection and membership \\nprotocol. Dynamo is a completely decentralized system with \\nminimal need for manual adminis tration. Storage nodes can be \\nadded and removed from Dynamo without requiring any manual \\npartitioning or redistribution. \\nIn the past year, Dynamo has been the underlying storage \\ntechnology for a number of the core services in Amazon’s e-\\ncommerce platform. It was able to scale to extreme peak loads \\nefficiently without any downtime during the busy holiday \\nshopping season. For example, the service that maintains \\nshopping cart (Shopping Cart Service)  served tens of millions \\nrequests that resulted in well over  3 million checkouts in a single \\nday and the service that mana ges session state handled hundreds \\nof thousands of concu rrently active sessions. \\nThe main contribution of this work for the research community is \\nthe evaluation of how different techniques can be combined to \\nprovide a single highly-available sy stem. It demonstrates that an \\neventually-consistent storage system can be used in production \\nwith demanding applications. It also provides insight into the \\ntuning of these techniques to m eet the requirements of production \\nsystems with very stri ct performance demands. \\nThe paper is structured as follows. Section 2 presents the \\nbackground and Section 3 presents  the related work. Section 4 \\npresents the system design and Section 5 describes the \\nimplementation. Secti on 6 details the experiences and insights \\ngained by running Dynamo in production and Section 7 concludes \\nthe paper. There are a number of places in this paper where \\nadditional information may have been appropriate but where \\nprotecting Amazon’s business intere sts require us to reduce some \\nlevel of detail. For this reason, the intra- and inter-datacenter \\nlatencies in section 6, the absolute  request rates in section 6.2 and \\noutage lengths and workloads in section 6.3 are provided through \\naggregate measures instead of absolute details. \\n2. BACKGROUND  \\nAmazon’s e-commerce platform is composed of hundreds of \\nservices that work in concert to deliver functionality ranging from \\nrecommendations to order fulfillment to fraud detection. Each \\nservice is exposed through a well defined interface and is \\naccessible over the network. These services are hosted in an \\ninfrastructure that consists of tens  of thousands of servers located \\nacross many data centers world-wide. Some of these services are \\nstateless (i.e., services which aggregate responses from other \\nservices) and some are stateful (i.e., a service that generates its \\nresponse by executing business logic on its state stored in \\npersistent store). \\nTraditionally production systems st ore their state in relational \\ndatabases. For many of the more  common usage patterns of state \\npersistence, however, a relational database is a solution that is far \\nfrom ideal. Most of these services only store and retrieve data by \\nprimary key and do not require the complex querying and \\nmanagement functionality offered by an RDBMS. This excess \\nfunctionality requires expensiv e hardware and highly skilled \\npersonnel for its operation, making it a very inefficient solution. \\nIn addition, the available replication technologies are limited and \\ntypically choose consistency ov er availability. Although many \\nadvances have been made in the r ecent years, it is still not easy to \\nscale-out databases or use smar t partitioning schemes for load \\nbalancing. This paper describes Dynamo, a highly available data storage \\ntechnology that addresses the needs of these important classes of \\nservices. Dynamo has a simple key/value interface, is highly \\navailable with a clearly defined consistency window, is efficient \\nin its resource usage, and has a simple scale out scheme to address \\ngrowth in data set size or request rates. Each service that uses \\nDynamo runs its own Dynamo instances.  \\n2.1 System Assumptions and Requirements \\nThe storage system for this cla ss of services has the following \\nrequirements: \\nQuery Model : simple read and write operations to a data item that \\nis uniquely identified by a key. State is stored as binary objects \\n(i.e., blobs) identified by uni que keys. No operations span \\nmultiple data items and there is no need for relational schema. \\nThis requirement is based on th e observation that a significant \\nportion of Amazon’s services can work with this simple query \\nmodel and do not need any rela tional schema. Dynamo targets \\napplications that need to store objects that are relatively small \\n(usually less than 1 MB).  \\nACID Properties: ACID ( Atomicity, Consiste ncy, Isolation, \\nDurability ) is a set of properties th at guarantee that database \\ntransactions are processed reliably. In the context of databases, a \\nsingle logical operation on the da ta is called a transaction. \\nExperience at Amazon has shown th at data stores that provide \\nACID guarantees tend to have poor  availability. This has been \\nwidely acknowledged by both the industry and academia [5]. \\nDynamo targets applications that operate with weaker consistency \\n(the “C” in ACID) if this results in high availability. Dynamo \\ndoes not provide any isolation gu arantees and permits only single \\nkey updates.   \\nEfficiency : The system needs to function on a commodity \\nhardware infrastructure. In Am azon’s platform, services have \\nstringent latency requi rements which are in general measured at \\nthe 99.9th percentile of the distribution. Given that state access \\nplays a crucial role in service operation the storage system must be capable of meeting such stringent SLAs (see Section 2.2 \\nbelow). Services must be able to configure Dynamo such that they \\nconsistently achieve their latency and thr oughput requirements. \\nThe tradeoffs are in performance, cost efficiency, availability, and \\ndurability guarantees.  \\nOther Assumptions: Dynamo is used only by Amazon’s internal \\nservices. Its operation environment is assumed to be non-hostile \\nand there are no security related requirements such as \\nauthentication and authorization. Moreover, since each service \\nuses its distinct instance of Dy namo, its initial design targets a \\nscale of up to hundreds of storage hosts. We will discuss the \\nscalability limitations of Dynamo and possible scalability related \\nextensions in later sections. \\n2.2 Service Level Agreements (SLA) \\nTo guarantee that the application can deliver its functionality in a \\nbounded time, each and every dependency in the platform needs \\nto deliver its functionality with even tighter bounds. Clients and \\nservices engage in a Service Level Agreement (SLA), a formally \\nnegotiated contract where a client and a service agree on several \\nsystem-related characteristics, which most prominently include \\nthe client’s expected request rate  distribution for a particular API \\nand the expected service latency under those conditions. An \\nexample of a simple SLA is a service guaranteeing that it will \\n196 \\n206 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n Document(id_='49cdc274-e597-4726-ad5c-55f71907d412', embedding=None, metadata={'page_label': '3', 'file_name': 'amazon-dynamo-sosp2007.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='ae78db70f54d15ec48c4ae67c15e04fb4c6d8c6faa25ff6c97f9c9f4e7497da4', text='provide a response within 300ms for 99.9% of its requests for a \\npeak client load of 500 requests per second. \\nIn Amazon’s decentralized service oriented infrastructure, SLAs \\nplay an important role. For exampl e a page request to one of the \\ne-commerce sites typically requi res the rendering engine to \\nconstruct its response by sending  requests to over 150 services. \\nThese services often have multiple dependencies, which \\nfrequently are other services, and as such it is not uncommon for \\nthe call graph of an application to have more than one level. To \\nensure that the page rendering engine can maintain a clear bound \\non page delivery each service within the call chain must obey its \\nperformance contract.  \\nFigure 1 shows an abstract view of the architecture of Amazon’s \\nplatform, where dynamic web content is generated by page \\nrendering components which in turn  query many other services. A \\nservice can use different data stor es to manage its state and these \\ndata stores are only accessible within its service boundaries. Some \\nservices act as aggregators by using several other services to \\nproduce a composite response. Typica lly, the aggregator services \\nare stateless, although they  use extensive caching. \\nA common approach in the industry for forming a performance \\noriented SLA is to describe it using average, median and expected \\nvariance. At Amazon we have f ound that these metrics are not \\ngood enough if the goal is to build a system where all customers \\nhave a good experience, rather than just the majority.  For \\nexample if extensive personalization techniques are used then \\ncustomers with longer histories require more processing which \\nimpacts performance at the high-e nd of the distribution. An SLA \\nstated in terms of mean or median response times will not address \\nthe performance of this important customer segment. To address \\nthis issue, at Amazon, SLAs are expressed and measured at the \\n99.9th percentile of the distribution.  The choice for 99.9% over an \\neven higher percentile has been made based on a cost-benefit \\nanalysis which demonstrated a significant increase in cost to \\nimprove performance that much. Experiences with Amazon’s production systems have shown that  this approach provides a \\nbetter overall experience compared to those systems that meet \\nSLAs defined based on the mean or median. \\nIn this paper there are many references to this 99.9th percentile of \\ndistributions, which reflects Am azon engineers’ relentless focus \\non performance from the perspective of the customers’ \\nexperience. Many papers report on averages, so these are included \\nwhere it makes sense for comparison purposes. Nevertheless, \\nAmazon’s engineering and optimiza tion efforts are not focused on \\naverages. Several techniques, such as the load balanced selection \\nof write coordinators, are purely targeted at controlling \\nperformance at the 99.9th percentile.   \\nStorage systems often play an im portant role in establishing a \\nservice’s SLA, especially if the business logic is relatively \\nlightweight, as is the case for many Amazon services. State \\nmanagement then becomes the ma in component of a service’s \\nSLA. One of the main design considerations for Dynamo is to \\ngive services control over thei r system properties, such as \\ndurability and consistency, and to let services make their own \\ntradeoffs between functionality, performance and cost-\\neffectiveness. \\n2.3 Design Considerations \\nData replication algorithms used in commercial systems \\ntraditionally perform synchronous replica coordination in order to \\nprovide a strongly consistent data access interface. To achieve this \\nlevel of consistency, these algorithms are forced to tradeoff the \\navailability of the data under cer tain failure scenarios. For \\ninstance, rather than dealing with the uncertainty of the \\ncorrectness of an answer, the data is made unavailable until it is \\nabsolutely certain that it is correct. From the very early replicated \\ndatabase works, it is well known that when dealing with the \\npossibility of network failures, strong consistency and high data \\navailability cannot be achieved si multaneously [2, 11]. As such \\nsystems and applications need to  be aware which properties can \\nbe achieved under which conditions. \\nFor systems prone to server and network failures, availability can \\nbe increased by using optimistic  replication techniques, where \\nchanges are allowed to propagate to replicas in the background, \\nand concurrent, disconnected work  is tolerated.  The challenge \\nwith this approach is that it can  lead to conflicting changes which \\nmust be detected and resolved.  This process of conflict resolution \\nintroduces two problems: when to resolve them and who resolves \\nthem. Dynamo is designed to be an eventually consistent data \\nstore; that is all updates re ach all replicas eventually. \\nAn important design consideration is to decide when  to perform \\nthe process of resolving update c onflicts, i.e., whether conflicts \\nshould be resolved during reads or  writes. Many traditional data \\nstores execute conflict resolution during writes and keep the read \\ncomplexity simple [7]. In such systems, writes may be rejected if \\nthe data store cannot reach all (or a majority of) the replicas at a \\ngiven time. On the other hand, Dynamo targets the design space \\nof an “always writeable” data store (i.e., a data store that is highly \\navailable for writes). For a number of Amazon services, rejecting \\ncustomer updates could result in a poor customer experience. For \\ninstance, the shopping cart servic e must allow customers to add \\nand remove items from their s hopping cart even amidst network \\nand server failures. This requ irement forces us to push the \\ncomplexity of conflict resolution to the reads in order to ensure \\nthat writes are never rejected.  \\n \\nFigure 1: Service-oriented architecture of Amazon’s \\nplatform \\n197 \\n207 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n Document(id_='0aeb7d67-f7f0-4e43-9c67-d14c8b775695', embedding=None, metadata={'page_label': '4', 'file_name': 'amazon-dynamo-sosp2007.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='3d47943c54433a70af4bc2594c532fbae667fbb3911c87b672e8182792f9a508', text='The next design choice is who performs the process of conflict \\nresolution. This can be done by the data store or the application. If \\nconflict resolution is done by the da ta store, its choices are rather \\nlimited. In such cases, the data store can only use simple policies, \\nsuch as “last write wins” [22], to resolve conflicting updates. On \\nthe other hand, since the application is aware of the data schema it \\ncan decide on the conflict resoluti on method that is best suited for \\nits client’s experience. For instance, the application that maintains \\ncustomer shopping carts can choos e to “merge” the conflicting \\nversions and return a single un ified shopping cart. Despite this \\nflexibility, some application de velopers may not want to write \\ntheir own conflict resolution me chanisms and choose to push it \\ndown to the data store, which in turn chooses a simple policy such \\nas “last write wins”.  \\nOther key principles embra ced in the design are: \\nIncremental scalability : Dynamo should be able to scale out one \\nstorage host (henceforth, referred to as “ node” ) at a time, with \\nminimal impact on both operators of the system and the system \\nitself. \\nSymmetry : Every node in Dynamo should have the same set of \\nresponsibilities as its peers; ther e should be no distinguished node \\nor nodes that take special roles or extra set of responsibilities. In \\nour experience, symmetry simplif ies the process of system \\nprovisioning and maintenance.  \\nDecentralization : An extension of symmetry, the design should \\nfavor decentralized peer-to-pe er techniques over centralized \\ncontrol. In the past, centralized control has resulted in outages and \\nthe goal is to avoid it as much as possible. This leads to a simpler, \\nmore scalable, and more available system. \\nHeterogeneity : The system needs to be able to exploit \\nheterogeneity in the infrastructur e it runs on. e.g. the work \\ndistribution must be proportional to the capabilities of the \\nindividual servers. This is e ssential in adding new nodes with \\nhigher capacity without having to upgrade all hosts at once. \\n3. RELATED WORK \\n3.1 Peer to Peer Systems \\nThere are several peer-to-peer (P 2P) systems that have looked at \\nthe problem of data storage and distribution. The first generation \\nof P2P systems, such as Freenet and Gnutella1, were \\npredominantly used as file sharing systems. These were examples \\nof unstructured P2P networks where the overlay links between \\npeers were established arbitrarily. In these networks, a search \\nquery is usually flooded through the network to find as many \\npeers as possible that share the data. P2P systems evolved to the \\nnext generation into what is widely known as structured P2P \\nnetworks. These networks employ a globally consistent protocol \\nto ensure that any node can efficiently route a search query to \\nsome peer that has the desired da ta. Systems like Pastry [16] and \\nChord [20] use routing mechanisms to ensure that queries can be \\nanswered within a bounded number of hops. To reduce the \\nadditional latency introduced by multi-hop routing, some P2P \\nsystems (e.g., [14]) employ O(1) routing where each peer \\nmaintains enough routing informati on locally so that it can route \\nrequests (to access a data item) to the appropriate peer within a \\nconstant number of hops.   Various storage systems, such as Oceanstore [9] and PAST [17] \\nwere built on top of these routin g overlays. Oceanstore provides a \\nglobal, transactional, persistent  storage service that supports \\nserialized updates on widely re plicated data. To allow for \\nconcurrent updates while avoiding many of the problems inherent \\nwith wide-area locking, it uses  an update model based on conflict \\nresolution. Conflict resolution was introduced in [21] to reduce \\nthe number of transaction aborts. Oceanstore resolves conflicts by \\nprocessing a series of updates, choosing a total order among them, \\nand then applying them atomically in that order. It is built for an \\nenvironment where the data is replicated on an untrusted \\ninfrastructure. By comparis on, PAST provides a simple \\nabstraction layer on top of Pastry  for persistent and immutable \\nobjects. It assumes that the application can build the necessary \\nstorage semantics (such as mutable files) on top of it.  \\n3.2 Distributed File Systems and Databases \\nDistributing data for performance, availability and durability has \\nbeen widely studied in the file system and database systems \\ncommunity. Compared to P2P storage systems that only support \\nflat namespaces, distributed file systems typically support \\nhierarchical namespaces. Systems like Ficus [15] and Coda [19] \\nreplicate files for high availability  at the expense of consistency. \\nUpdate conflicts are typically ma naged using specialized conflict \\nresolution procedures. The Farsite sy stem [1] is a distributed file \\nsystem that does not use any centralized server like NFS. Farsite \\nachieves high availability and scal ability using re plication. The \\nGoogle File System [6] is another distributed file system built for \\nhosting the state of Google’s inte rnal applications. GFS uses a \\nsimple design with a single master server for hosting the entire \\nmetadata and where the data is split into chunks and stored in \\nchunkservers. Bayou is a distribut ed relational database system \\nthat allows disconnected operations  and provides eventual data \\nconsistency [21].  \\nAmong these systems, Bayou, Coda  and Ficus allow disconnected \\noperations and are resilient to issu es such as network partitions \\nand outages. These systems differ on their conflict resolution \\nprocedures. For instance, Coda and Ficus perform system level \\nconflict resolution and Bayou allows application level resolution. \\nAll of them, however, guarantee ev entual consistency. Similar to \\nthese systems, Dynamo allows read and write operations to \\ncontinue even during network partitions and resolves updated \\nconflicts using different co nflict resolution mechanisms. \\nDistributed block storage systems like FAB [18] split large size \\nobjects into smaller blocks and stores each block in a highly \\navailable manner. In comparison to these systems, a key-value \\nstore is more suitable in this case because: (a) it is intended to \\nstore relatively small objects (siz e < 1M) and (b) key-value stores \\nare easier to configure on a per-a pplication basis. Antiquity is a \\nwide-area distributed storage syst em designed to handle multiple \\nserver failures [23]. It uses a secure  log to preserve data integrity, \\nreplicates each log on multiple servers for durability, and uses \\nByzantine fault tolerance protocols to ensure data consistency. In \\ncontrast to Antiquity, Dynamo does not focus on the problem of \\ndata integrity and security and is built for a trusted environment. \\nBigtable is a distributed storag e system for managing structured \\ndata. It maintains a sparse, multi-dimensional sorted map and \\nallows applications to access their data using multiple attributes \\n[2]. Compared to Bigtable, Dynamo targets applications that \\nrequire only key/value access with primary focus on high \\navailability where updates are not re jected even in the wake of \\nnetwork partitions or server failures. 1 http://freenetproject.or g/, http://www.gnutella.org \\n198 \\n208 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n Document(id_='aa5431e2-cbed-455c-a348-a3afb3e5ed29', embedding=None, metadata={'page_label': '5', 'file_name': 'amazon-dynamo-sosp2007.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='d66b6153d05ae657e31e239dd4c2b779e01d175d36302dbc3ea061a05bbbfd5c', text='Traditional replicated relationa l database systems focus on the \\nproblem of guaranteeing strong c onsistency to replicated data. \\nAlthough strong consistency provi des the application writer a \\nconvenient programming model, these systems are limited in \\nscalability and availability [7]. These systems are not capable of \\nhandling network partitions because  they typically provide strong \\nconsistency guarantees.  \\n3.3 Discussion \\nDynamo differs from the aforementioned decentralized storage \\nsystems in terms of its target requirements. Firs t, Dynamo is \\ntargeted mainly at applications that need an “always writeable” \\ndata store where no updates are rejected due to failures or \\nconcurrent writes. This is a crucial requirement for many Amazon \\napplications. Second, as noted earlier, Dynamo is built for an \\ninfrastructure within a single administrative domain where all \\nnodes are assumed to be trusted. Third, applications that use \\nDynamo do not require support for hierarchical namespaces (a \\nnorm in many file systems) or complex relational schema \\n(supported by traditional databases). Fourth, Dynamo is built for \\nlatency sensitive applic ations that require at least 99.9% of read \\nand write operations to be performed within a few hundred \\nmilliseconds. To meet these string ent latency requ irements, it was \\nimperative for us to avoid routing requests through multiple nodes \\n(which is the typical design adop ted by several distributed hash \\ntable systems such as Chord and Pastry). This is because multi-\\nhop routing increases variabilit y in response times, thereby \\nincreasing the latency at higher  percentiles. Dynamo can be \\ncharacterized as a zero-hop DHT, where each node maintains \\nenough routing information locally to route a request to the \\nappropriate node directly. \\n4. SYSTEM ARCHITECTURE \\nThe architecture of a storage system that needs to operate in a \\nproduction setting is complex. In addition to the actual data \\npersistence component, the system needs to have scalable and \\nrobust solutions for load balancing, membership and failure \\ndetection, failure recovery, re plica synchroni zation, overload \\nhandling, state transfer, concurrency and job scheduling, request \\nmarshalling, request routing, sy stem monitoring and alarming, \\nand configuration management. Describing the details of each of \\nthe solutions is not possible, so this paper focuses on the core \\ndistributed systems techniques used in Dynamo: partitioning, \\nreplication, versioning,  membership, failure handling and scaling. Table 1 presents a summary of the list of techniques Dynamo uses \\nand their respective advantages. \\n4.1 System Interface  \\nDynamo stores objects associat ed with a key through a simple \\ninterface; it exposes two operations: get() and put(). The get( key) \\noperation locates the object replicas associated with the key in the \\nstorage system and returns a single object  or a list of objects with \\nconflicting versions along with a context . The put( key, context, \\nobject ) operation determines where the replicas of the object \\nshould be placed based on the associated key, and writes the \\nreplicas to disk. The context  encodes system metadata about the \\nobject that is opaque to the caller  and includes information such as \\nthe version of the object. The context information is stored along \\nwith the object so that the system can verify the validity of the \\ncontext object supplied in the put request. \\nDynamo treats both the key and the object supplied by the caller \\nas an opaque array of bytes. It  applies a MD5 hash on the key to \\ngenerate a 128-bit identifier, wh ich is used to determine the \\nstorage nodes that are responsible for serving the key.  \\n4.2 Partitioning Algorithm \\nOne of the key design requirements for Dynamo is that it must \\nscale incrementally. This requires a mechanism to dynamically \\npartition the data over the set of  nodes (i.e., storage hosts) in the \\nsystem. Dynamo’s partitioning scheme relies on consistent \\nhashing to distribute the load across multiple storage hosts. In \\nconsistent hashing [10], the output  range of a hash function is \\ntreated as a fixed circular space or “ring” (i.e. the largest hash \\nvalue wraps around to the smallest hash value). Each node in the \\nsystem is assigned a random value within this space which \\nrepresents its “position” on the ring.  Each data item identified by \\na key is  assigned to a node by hash ing the data item’s key to yield \\nits position on the ring, and then walking the ring clockwise to \\nfind the first node with a position larger than the item’s position. A \\nB \\nC \\nD E F G Key K \\nNodes B, C \\nand D store \\nkeys in \\nrange (A,B) \\nincluding \\nK. \\n \\n \\nFigure 2: Partitioning and replication of keys in Dynamo \\nring.  Table 1: Summary of techniques used in Dynamo  and \\ntheir advantages. \\nProblem Technique Advantage \\nPartitioning Consistent Hashing Incremental \\nScalability \\nHigh Availability \\nfor writes Vector clocks with \\nreconciliation during \\nreads Version size is \\ndecoupled from \\nupdate rates. \\nHandling temporary \\nfailures Sloppy Quorum and \\nhinted handoff Provides high \\navailability and \\ndurability guarantee \\nwhen some of the \\nreplicas are not \\navailable. \\nRecovering from \\npermanent failures Anti-entropy using \\nMerkle trees Synchronizes \\ndivergent replicas in \\nthe background. \\nMembership and \\nfailure detection Gossip-based \\nmembership protocol \\nand failure detection. Preserves symmetry \\nand avoids having a \\ncentralized registry \\nfor storing \\nmembership and \\nnode liveness \\ninformation. \\n199 \\n209 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n Document(id_='c8e9d0f4-a57a-436e-a32d-64ce7e6a67e6', embedding=None, metadata={'page_label': '6', 'file_name': 'amazon-dynamo-sosp2007.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='1f06d4c49e0912d54b51623162f3f9995e36366bf38a98dad46da1b529e2e1a2', text='Thus, each node becomes responsible for the region in the ring \\nbetween it and its predecessor node on the ring. The principle \\nadvantage of consistent hashing is  that departure or arrival of a \\nnode only affects its immediate neighbors a nd other nodes remain \\nunaffected.  \\nThe basic consistent hashing algo rithm presents some challenges. \\nFirst, the random position assignme nt of each node on the ring \\nleads to non-uniform data and load  distribution. Second, the basic \\nalgorithm is oblivious to the hete rogeneity in the performance of \\nnodes. To address these issues, Dynamo uses a variant of \\nconsistent hashing (similar to the one  used in [10, 20]): instead of \\nmapping a node to a single point in the circle, each node gets \\nassigned to multiple points in the ring. To this end, Dynamo uses \\nthe concept of “virtual nodes”. A virtual node looks like a single \\nnode in the system, but each node can be responsible for more \\nthan one virtual node. Effectively,  when a new node is added to \\nthe system, it is assigned multiple positions (henceforth, “tokens”) \\nin the ring. The process of fine-tuning Dynamo’s partitioning \\nscheme is discussed in Section 6. \\nUsing virtual nodes has the following advantages: \\n• If a node becomes unavailable (due to failures or routine \\nmaintenance), the load handled by this node is evenly dispersed across the rema ining available nodes. \\n• When a node becomes available again, or a new node is \\nadded to the system, the newly available node accepts a \\nroughly equivalent amount of load from each of the other \\navailable nodes.  \\n• The number of virtual nodes th at a node is responsible can \\ndecided based on its capacity, accounting for heterogeneity \\nin the physical infrastructure.  \\n4.3 Replication \\nTo achieve high availability and durability, Dynamo replicates its \\ndata on multiple hosts. Each data item is replicated at N hosts, \\nwhere N is a parameter configured “per-instance” . Each key, k, is \\nassigned to a coordinator node (descr ibed in the previous section). \\nThe coordinator is in charge of the replication of the data items \\nthat fall within its range. In addition to locally storing each key \\nwithin its range, the coordinator re plicates these keys at the N-1 \\nclockwise successor nodes in the ri ng. This results in a system \\nwhere each node is responsible for the region of the ring between \\nit and its Nth predecessor. In Figure 2, node B replicates the key k \\nat nodes C and D in addition to storing it locally. Node D will \\nstore the keys that fall in the ranges (A, B], (B, C], and (C, D]. \\nThe list of nodes that is responsible  for storing a particular key is \\ncalled the preference list . The system is designed, as will be \\nexplained in Section 4.8, so th at every node in the system can \\ndetermine which nodes should be in  this list for any particular \\nkey.  To account for node failures, preference list contains more \\nthan N nodes. Note that with the us e of virtual nodes, it is possible \\nthat the first N successor positions  for a particular key may be \\nowned by less than N distinct physical nodes (i.e. a node may \\nhold more than one of the first N positions). To address this, the \\npreference list for a key is constr ucted by skipping positions in the \\nring to ensure that the list contai ns only distinct physical nodes.  \\n4.4 Data Versioning  \\nDynamo provides eventual consis tency, which allows for updates \\nto be propagated to all repli cas asynchronously. A put() call may return to its caller before the update has been applied at all the \\nreplicas, which can result in scenarios where a subsequent get() \\noperation may return an object th at does not have the latest \\nupdates.. If there are no failures then there is a bound on the \\nupdate propagation times. However, under certain failure \\nscenarios (e.g., server outages or  network partitions), updates may \\nnot arrive at all re plicas for an extended period of time. \\nThere is a category of applications in Amazon’s platform that can \\ntolerate such inconsistencies and can be constructed to operate \\nunder these conditions. For example,  the shopping cart application \\nrequires that an “ Add to Cart”  operation can never be forgotten or \\nrejected. If the most recent state of the cart is unavailable, and a \\nuser makes changes to an older version of the cart, that change is \\nstill meaningful and should be preserved. But at the same time it \\nshouldn’t supersede the currently unavailable state of the cart, \\nwhich itself may contain changes that should be preserved. Note \\nthat both “ add to cart ” and “ delete item from cart ” operations are \\ntranslated into put requests to Dynamo. When a customer wants to \\nadd an item to (or remove from) a shopping cart and the latest \\nversion is not available, the ite m is added to (or removed from) \\nthe older version and the divergent versions are reconciled later.  \\nIn order to provide this kind of guarantee, Dynamo treats the \\nresult of each modification as a new and immutable version of the \\ndata. It allows for multiple versions  of an object to be present in \\nthe system at the same time. Most of the time, new versions \\nsubsume the previous version( s), and the system itself can \\ndetermine the authoritative version (syntactic reconciliation).  \\nHowever, version branching ma y happen, in the presence of \\nfailures combined with concurrent updates, resulting in \\nconflicting versions of an object. In these cases, the system cannot \\nreconcile the multiple versions of the same object and the client \\nmust perform the reconciliation in order to collapse  multiple \\nbranches of data evolution back into one (semantic \\nreconciliation). A typical example of a collapse operation is \\n“merging” different versions of a customer’s shopping cart. Using \\nthis reconciliation mechanism, an “add to cart” operation is never lost. However, deleted items can resurface. \\nIt is important to understand that certain failure modes can \\npotentially result in the system  having not just two but several \\nversions of the same data. Upda tes in the presence of network \\npartitions and node failures can potentially result in an object \\nhaving distinct version sub-histor ies, which the system will need \\nto reconcile in the future. This requires us to design applications \\nthat explicitly acknowledge the possi bility of multiple versions of \\nthe same data (in order to never lose any updates).  \\nDynamo uses vector clocks [12] in order to capture causality \\nbetween different versions of th e same object. A vector clock is \\neffectively a list of (node, counter) pairs. One vector clock is \\nassociated with every version of every object. One can determine \\nwhether two versions of an object are on parallel branches or have \\na causal ordering, by examine thei r vector clocks. If the counters \\non the first object’s clock are less-than-or-equal to all of the nodes \\nin the second clock, then the firs t is an ancestor of the second and \\ncan be forgotten. Otherwise, the two changes are considered to be \\nin conflict and require reconciliation. \\nIn Dynamo, when a client wishes  to update an object, it must \\nspecify which version it is updating. This is done by passing the \\ncontext it obtained from an earlier read operation, which contains \\nthe vector clock information. Upon processing a read request, if \\n200 \\n210 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n Document(id_='c41797c9-5456-4127-9733-fa3d0465ac40', embedding=None, metadata={'page_label': '7', 'file_name': 'amazon-dynamo-sosp2007.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='66f54c03e77f8b8de1c0b5e4449c4d1e325008b374cd1fe83330c556616f5ea8', text=\"Dynamo has access to multiple branches that cannot be \\nsyntactically reconciled, it will return all the objects at the leaves, with the corresponding version information in the context. An \\nupdate using this context is cons idered to have reconciled the \\ndivergent versions and the branches are collapsed into a single \\nnew version. \\nTo illustrate the use of vector cl ocks, let us consider the example \\nshown in Figure 3.  A client writes a new object. The node (say \\nSx) that handles the write for this key increases its sequence \\nnumber and uses it to create the da ta's vector clock. The system \\nnow has the object D1 and its associated clock [(Sx, 1)]. The \\nclient updates the objec t. Assume the same  node handles this \\nrequest as well. The system now  also has object D2 and its \\nassociated clock [(Sx, 2)]. D2 descends  from D1 and therefore \\nover-writes D1, however there may be  replicas of D1 lingering at \\nnodes that have not yet seen D2. Let us assume that the same \\nclient updates the object again and a different server (say Sy) \\nhandles the request. The system  now has data D3 and its \\nassociated clock [(Sx, 2), (Sy, 1)].  \\nNext assume a different client read s D2 and then tries to update it, \\nand another node (say Sz) does the write. The system now has D4 \\n(descendant of D2) whose version clock is [(Sx, 2), (Sz, 1)]. A \\nnode that is aware of D1 or D2 could determine, upon receiving \\nD4 and its clock, that D1 and D2 are overwritten by the new data \\nand can be garbage collected. A node that is aware of D3 and \\nreceives D4 will find that there is no causal relation between \\nthem. In other words, there are changes in D3 and D4 that are not \\nreflected in each other. Both versi ons of the data must be kept and \\npresented to a client (upon a read ) for semantic reconciliation.  \\n Now assume some client reads both D3 and D4 (the context will \\nreflect that both values were found by the read). The read's \\ncontext is a summary of the clocks  of D3 and D4, namely [(Sx, 2), \\n(Sy, 1), (Sz, 1)]. If the client performs the reconciliation and node \\nSx coordinates the write, Sx wi ll update its sequence number in \\nthe clock. The new data D5 will have the following clock: [(Sx, \\n3), (Sy, 1), (Sz, 1)].  \\nA possible issue with vector clocks is that the size of vector \\nclocks may grow if many servers coordinate the writes to an object. In practice, this is not likely because the writes are usually \\nhandled by one of the top N nodes in the preference list. In case of \\nnetwork partitions or multiple server failures, write requests may \\nbe handled by nodes that are not in the top N nodes in the \\npreference list causing the size of vector clock to grow. In these \\nscenarios, it is desirable to limit the size of vector clock. To this \\nend, Dynamo employs the follo wing clock truncation scheme: \\nAlong with each (node, counter) pair, Dynamo stores a timestamp \\nthat indicates the last time the node updated the data item. When \\nthe number of (node, counter) pairs in the vector clock reaches a \\nthreshold (say 10), the oldest pa ir is removed from the clock. \\nClearly, this truncation scheme can lead to inefficiencies in \\nreconciliation as the descendant relationships cannot be derived \\naccurately. However, this problem has not surfaced in production \\nand therefore this issue has no t been thoroughly investigated.  \\n4.5 Execution of get () and put () operations \\nAny storage node in Dynamo is eligible to receive client get and \\nput operations for any key. In this section, for sake of simplicity, \\nwe describe how these operations  are performed in a failure-free \\nenvironment and in the subsequent  section we describe how read \\nand write operations are executed during failures.  \\nBoth get and put operations are invoked using Amazon’s \\ninfrastructure-specific request processing framework over HTTP. \\nThere are two strategies that a client can use to select a node: (1) \\nroute its request through a generic load balancer that will select a \\nnode based on load information, or (2) use a partition-aware client \\nlibrary that routes requests directly to the appropriate coordinator \\nnodes. The advantage of the first a pproach is that the client does \\nnot have to link any code specific to Dynamo in its application, \\nwhereas the second strategy can achieve lower latency because it \\nskips a potential forwarding step. \\nA node handling a read or write operation is known as the \\ncoordinator . Typically, this is the first among the top N nodes in \\nthe preference list. If the requests are received through a load \\nbalancer, requests to access a key may be routed to any random node in the ring. In this scen ario, the node that receives the \\nrequest will not coordinate it if the node is not in the top N of the \\nrequested key’s preference list. In stead, that node will forward the \\nrequest to the first among the top N nodes in the preference list. \\n Read and write operations involve the first N healthy nodes in the \\npreference list, skipping over those that are down or inaccessible. \\nWhen all nodes are healthy, the top N nodes in a key’s preference \\nlist are accessed. When there are node failures or network \\npartitions, nodes that are lower ranked in the preference list are \\naccessed.  \\nTo maintain consistency among its replicas, Dynamo uses a \\nconsistency protocol similar to those used in quorum systems. \\nThis protocol has two key configurable values: R and W. R is the \\nminimum number of nodes that must participate in a successful \\nread operation. W is the mini mum number of nodes that must \\nparticipate in a successful write operation.  Setting R and W such \\nthat R + W > N yields a quorum-lik e system. In this model, the \\nlatency of a get (or put) operation is dictated by the slowest of the \\nR (or W) replicas. For this reason, R and W are usually \\nconfigured to be less than N, to provide better latency.  \\nUpon receiving a put() request for a key, the coordinator generates \\nthe vector clock for the new ve rsion and writes the new version \\nlocally. The coordinator then se nds the new version (along with \\n \\nFigure 3: Version evolutio n of an object over time. \\n201 \\n211 \", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n Document(id_='edaffbac-dc65-4e87-a508-7659f2c8e1e5', embedding=None, metadata={'page_label': '8', 'file_name': 'amazon-dynamo-sosp2007.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='2141c87414fbb27368acde909b8920a8523d6a7e442e48a18c6d324a6ba1fc71', text='the new vector clock) to the N highest-ranked reachable nodes. If \\nat least W-1 nodes respond then the write is considered \\nsuccessful.  \\nSimilarly, for a get() request, the coordinator requests all existing \\nversions of data for that key from the N highest-ranked reachable \\nnodes in the preference list for that key, and then waits for R \\nresponses before returning th e result to the client. If the \\ncoordinator ends up gathering multiple versions of the data, it \\nreturns all the versions it deems to be causally unrelated. The \\ndivergent versions are then reconciled and the reconciled version \\nsuperseding the current vers ions is written back.  \\n4.6 Handling Failures: Hinted Handoff \\nIf Dynamo used a traditional quorum approach it would be \\nunavailable during server failure s and network partitions, and \\nwould have reduced durability even  under the simplest of failure \\nconditions. To remedy this it does not enforce strict quorum \\nmembership and instead it uses a “sloppy quorum”; all read and \\nwrite operations are performed on the first N healthy  nodes from \\nthe preference list, which may not always be the first N nodes \\nencountered while walking the consistent hashing ring.  \\nConsider the example of Dynamo configuration given in Figure 2 \\nwith N=3. In this example, if  node A is temporarily down or \\nunreachable during a write operation then a replica that would \\nnormally have lived on A will now be  sent to node D. This is done \\nto maintain the desired availability and durability guarantees. The \\nreplica sent to D will have a hint  in its metadata that suggests \\nwhich node was the intended recipien t of the replic a (in this case \\nA).  Nodes that receive hinted replicas will keep them in a \\nseparate local database that is scanned periodically. Upon \\ndetecting that A has recovered, D will attempt to deliver the \\nreplica to A.  Once the transfer succeeds, D may delete the object \\nfrom its local store without decreasing the total number of replicas \\nin the system.  \\nUsing hinted handoff, Dynamo en sures that the read and write \\noperations are not failed due to temporary node or network failures. Applications that need the highest level of availability \\ncan set W to 1, which ensures that  a write is accepted as long as a \\nsingle node in the system has durably written the key it to its local \\nstore. Thus, the write request is only rejected if all nodes in the \\nsystem are unavailable. However, in practice, most Amazon \\nservices in production set a higher W to meet the desired level of \\ndurability. A more detailed discu ssion of configuring N, R and W \\nfollows in section 6.  \\nIt is imperative that a highly av ailable storage sy stem be capable \\nof handling the failure of an entir e data center(s). Data center \\nfailures happen due to power outa ges, cooling failures, network \\nfailures, and natural disasters. Dy namo is configured such that \\neach object is replicated across multiple data centers. In essence, \\nthe preference list of a key is constructed such that the storage \\nnodes are spread across multiple data centers. These datacenters \\nare connected through high speed network links. This scheme of \\nreplicating across multiple datacenters allows us to handle entire \\ndata center failures w ithout a data outage.  \\n4.7 Handling permanent failures: Replica \\nsynchronization \\nHinted handoff works best if the system membership churn is low \\nand node failures are transient. There are scenarios under which \\nhinted replicas become unavailable before they can be returned to the original replica node. To handle this and other threats to \\ndurability, Dynamo implements an anti-entropy (replica \\nsynchronization) protocol to keep  the replicas synchronized.   \\nTo detect the inconsistencies be tween replicas faster and to \\nminimize the amount of transferred data, Dynamo uses Merkle \\ntrees [13]. A Merkle tree is a hash tree where leaves are hashes of \\nthe values of individual keys. Pare nt nodes higher in the tree are \\nhashes of their respective children. The principal advantage of \\nMerkle tree is that each branch of the tree can be checked \\nindependently without requiring nodes to download the entire tree \\nor the entire data set. Moreover, Merkle trees help in reducing the \\namount of data that needs to be transferred while checking for \\ninconsistencies among replicas. For in stance, if the hash values of \\nthe root of two trees are equal, then the values of the leaf nodes in \\nthe tree are equal and the nodes require no synchronization. If not, \\nit implies that the values of some replicas are different. In such \\ncases, the nodes may exchange the hash values of children and the \\nprocess continues until it reaches the leaves of the trees, at which \\npoint the hosts can identify the keys that are “out of sync”. Merkle \\ntrees minimize the amount of data that needs to be transferred for \\nsynchronization and reduce the num ber of disk reads performed \\nduring the anti-entropy process.  \\nDynamo uses Merkle trees for an ti-entropy as follows: Each node \\nmaintains a separate Merkle tree for each key range (the set of \\nkeys covered by a virtual node) it hosts. This allows nodes to \\ncompare whether the keys within a key range are up-to-date. In \\nthis scheme, two nodes exchange the root of the Merkle tree \\ncorresponding to the key ranges that they host in common. \\nSubsequently, using the tree trav ersal scheme desc ribed above the \\nnodes determine if they have any differences and perform the \\nappropriate synchronization action. The disadvantage with this \\nscheme is that many key ranges change when a node joins or \\nleaves the system thereby requiring the tree(s) to be recalculated. \\nThis issue is addressed, however , by the refined partitioning \\nscheme described in Section 6.2. \\n4.8 Membership and Failure Detection \\n4.8.1 Ring Membership \\nIn Amazon’s environment node outages (due to failures and \\nmaintenance tasks) are often transient but may last for extended \\nintervals.  A node outage rarely signifies a permanent departure \\nand therefore should not result in  rebalancing of the partition \\nassignment or repair of the unreachable replicas.  Similarly, \\nmanual error could result in the unintentional startup of new \\nDynamo nodes.   For these reasons , it was deemed appropriate to \\nuse an explicit mechanism to initiate the addition and removal of \\nnodes from a Dynamo ring. An administrator uses a command line tool or a browser to connect to a Dynamo node and issue a \\nmembership change to join a node to a ring or remove a node \\nfrom a ring.  The node that serves the request writes the \\nmembership change and its time of issue to persistent store. The \\nmembership changes form a history because nodes can be \\nremoved and added back multiple  times. A gossip-based protocol \\npropagates membership changes and maintains an eventually \\nconsistent view of membership. Each node contacts a peer chosen \\nat random every second and the two nodes efficien tly reconcile \\ntheir persisted membership change histories.   \\nWhen a node starts for the first tim e, it chooses its set of tokens \\n(virtual nodes in the consistent hash space) and maps nodes to \\ntheir respective token sets. The mapping is persisted on disk and \\n202 \\n212 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n Document(id_='74c6e2dd-0998-46ef-8934-f8f120d43843', embedding=None, metadata={'page_label': '9', 'file_name': 'amazon-dynamo-sosp2007.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='55dac0fefe1a592fb40f86dccc1688edad2fe8365622fb14e4e99aff39a14972', text=\"initially contains only the local n ode and token set.  The mappings \\nstored at different Dynamo nodes are reconciled during the same \\ncommunication exchange that reconciles the membership change \\nhistories. Therefore, partitioning and placement information also \\npropagates via the gossip-based pr otocol and each storage node is \\naware of the token ranges handled by its peers. This allows each \\nnode to forward a key’s read/write operations to the right set of \\nnodes directly.  \\n4.8.2 External Discovery \\nThe mechanism described above could temporarily result in a \\nlogically partitioned Dynamo  ring.  For example, the \\nadministrator could contact node A to join A to the ring, then \\ncontact node B to join B to the ring. In this scenario, nodes A and \\nB would each consider itself a member of the ring, yet neither \\nwould be immediately aware of th e other.  To prevent logical \\npartitions, some Dynamo nodes play the role of seeds.  Seeds are \\nnodes that are discovered via an external mechanism and are \\nknown to all nodes.  Because all nodes eventually reconcile their \\nmembership with a seed, logical partitions are highly unlikely.  \\nSeeds can be obtained either from static configuration or from a \\nconfiguration service.  Typically seeds are fully functional nodes \\nin the Dynamo ring. \\n4.8.3 Failure Detection \\nFailure detection in Dynamo is used to avoid attempts to \\ncommunicate with unreachable peers during get() and put() \\noperations and when transferring pa rtitions and hinted replicas.  \\nFor the purpose of avoiding failed  attempts at communication, a \\npurely local notion of failure det ection is entirely sufficient: node \\nA may consider node B failed if node B does not respond to node \\nA’s messages (even if B is responsive to node C's messages).  In \\nthe presence of a steady rate of client requests generating inter-\\nnode communication in the Dynamo ring, a node A quickly \\ndiscovers that a node B is unresponsive when B fails to respond to \\na message; Node A then uses al ternate nodes to service requests \\nthat map to B's partitions; A periodically retries B to check for the \\nlatter's recovery.  In the absence of client requests to drive traffic \\nbetween two nodes, neither node really needs to know whether the \\nother is reachable and responsive. \\nDecentralized failure detection protocols use a simple gossip-style \\nprotocol that enable each node in the system to learn about the \\narrival (or departure) of other no des. For detailed information on \\ndecentralized failure detectors and the parameters affecting their \\naccuracy, the interested reader is referred to [8]. Early designs of \\nDynamo used a decentralized failure detector to maintain a \\nglobally consistent view of failure  state.  Later it was determined \\nthat the explicit node join and le ave methods obviates the need for \\na global view of failure state. This is because nodes are notified of permanent node additions and remova ls by the explicit node join \\nand leave methods and temporary node failures are detected by \\nthe individual nodes when they fail to communicate with others \\n(while forwarding requests).  \\n4.9 Adding/Removing Storage Nodes \\nWhen a new node (say X) is added into the system, it gets \\nassigned a number of tokens that are randomly scattered on the \\nring. For every key range that is assigned to node X, there may be \\na number of nodes (less than or equal to N) that are currently in \\ncharge of handling keys that fall within its token range. Due to the \\nallocation of key ranges to X, so me existing nodes no longer have \\nto some of their keys and these nodes transfer those keys to X. Let us consider a simple bootstrapping scenario where node X is \\nadded to the ring shown in Figure 2 between A and B. When X is \\nadded to the system, it is in charge of storing keys in the ranges \\n(F, G], (G, A] and (A, X]. As a consequence, nodes B, C and D no \\nlonger have to store the keys in these respective ranges. \\nTherefore, nodes B, C, and D wi ll offer to and upon confirmation \\nfrom X transfer the appropriate set of keys.  When a node is \\nremoved from the system, the real location of keys happens in a \\nreverse process.   \\nOperational experience has shown that this approach distributes \\nthe load of key distribution uniformly across the storage nodes, \\nwhich is important to meet the latency requirements and to ensure \\nfast bootstrapping. Finally, by adding a confirmation round \\nbetween the source and the destination, it is made sure that the \\ndestination node does not receiv e any duplicate transfers for a \\ngiven key range.  \\n5. IMPLEMENTATION \\nIn Dynamo, each storage node has three main software \\ncomponents: request coordinati on, membership and failure \\ndetection, and a local persistenc e engine.  All these components \\nare implemented in Java.  \\nDynamo’s local persistence co mponent allows for different \\nstorage engines to be plugged in. Engines that are in use are \\nBerkeley Database (BDB) Transactional Data Store2, BDB Java \\nEdition, MySQL, and an in-memory buffer with persistent \\nbacking store. The main reason for designing a pluggable \\npersistence component is to choose  the storage engine best suited \\nfor an application’s access patterns. For instance, BDB can handle \\nobjects typically in the order of tens of kilobytes whereas MySQL \\ncan handle objects of larger si zes. Applications choose Dynamo’s \\nlocal persistence engine based on their object size distribution. \\nThe majority of Dynamo’s production instances use BDB \\nTransactional Data Store. \\nThe request coordination component is built on top of an event-\\ndriven messaging substrate where the message processing pipeline is split into multiple stages similar to the SEDA architecture [24]. \\nAll communications are implemen ted using Java NIO channels. \\nThe coordinator executes the read and write requests on behalf of \\nclients by collecting data from one  or more nodes (in the case of \\nreads) or storing data at one or more nodes (for writes). Each \\nclient request results in the crea tion of a state machine on the node \\nthat received the client request. The state machine contains all the \\nlogic for identifying the nodes responsible for a key, sending the \\nrequests, waiting for response s, potentially doing retries, \\nprocessing the replies and packag ing the response to the client. \\nEach state machine instance handles exactly one client request. \\nFor instance, a read operation implements the following state \\nmachine: (i) send read requests to the nodes, (ii) wait for \\nminimum number of required responses, (iii) if too few replies \\nwere received within a given time bound, fail the request, (iv) \\notherwise gather all the data versions and determine the ones to be \\nreturned and (v) if versioning is enabled, perform syntactic \\nreconciliation and generate an opaque write context that contains \\nthe vector clock that subsumes a ll the remaining versions. For the \\nsake of brevity the failure handli ng and retry states are left out. \\nAfter the read response has been returned to the caller the state \\n2 http://www.oracle.com/dat abase/berkeley-db.html \\n203 \\n213 \", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n Document(id_='dc1a3479-d15c-4fe1-8b1f-26ff1d105239', embedding=None, metadata={'page_label': '10', 'file_name': 'amazon-dynamo-sosp2007.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='f8a7b12dfdf265749d46a99c5ff40a842c7890cb1f702ee4a1c8a5906861a34e', text='machine waits for a small period of time to receive any \\noutstanding responses. If stale versi ons were returned in any of \\nthe responses, the coordinator upda tes those nodes with the latest \\nversion. This process is called read repair  because it repairs \\nreplicas that have missed a recent update at an opportunistic time \\nand relieves the anti-entropy protocol from having to do it.  \\nAs noted earlier, write requests ar e coordinated by one of the top \\nN nodes in the preference list. A lthough it is desirable always to \\nhave the first node among the top N to coordinate the writes \\nthereby serializing all writes at a single location, this approach has \\nled to uneven load distribution resu lting in SLA violations. This is \\nbecause the request load is not uniformly distributed across \\nobjects. To counter this, any of the top N nodes in the preference \\nlist is allowed to coordinate the writes. In particular, since each \\nwrite usually follows a read opera tion, the coordinator for a write \\nis chosen to be the node that rep lied fastest to the previous read \\noperation which is stored in th e context information of the \\nrequest. This optimization enables us to pick the node that has the \\ndata that was read by the preceding read operation thereby \\nincreasing the chances of getting “read-your-writes” consistency. \\nIt also reduces variability in the performance of the request \\nhandling which improves the performance at the 99.9 percentile.  \\n6. EXPERIENCES & LESSONS LEARNED \\nDynamo is used by several servi ces with different configurations. \\nThese instances differ by their version reconciliation logic, and \\nread/write quorum characteristics. The following are the main \\npatterns in which Dynamo is used: \\n• Business logic specific reconciliation:  This is a popular use \\ncase for Dynamo. Each data object is replicated across multiple nodes. In case of dive rgent versions, the client \\napplication performs its ow n reconciliation logic. The \\nshopping cart service discussed ear lier is a prime example of \\nthis category. Its business l ogic reconciles objects by \\nmerging different versions of  a customer’s shopping cart.  • Timestamp based reconciliation:  This case differs from the \\nprevious one only in the reconc iliation mechanism. In case of \\ndivergent versions, Dynamo performs simple timestamp \\nbased reconciliation logic of “las t write wins”; i.e., the object \\nwith the largest physical timestamp value is chosen as the \\ncorrect version. The service that maintains customer’s \\nsession information is a good exam ple of a service that uses \\nthis mode.  \\n• High performance read engine:  While Dynamo is built to be \\nan “always writeable” data store, a few services are tuning its quorum characteristics and us ing it as a high performance \\nread engine. Typically, these services have a high read \\nrequest rate and only a small number of updates. In this \\nconfiguration, typically R is se t to be 1 and W to be N. For \\nthese services, Dynamo provides the ability to partition and \\nreplicate their data across multiple nodes thereby offering \\nincremental scalability. Some of these instances function as \\nthe authoritative persistence cache for data stored in more \\nheavy weight backing stores. Se rvices that maintain product \\ncatalog and promotional items fit in this category. \\nThe main advantage of Dynamo is that its client applications can \\ntune the values of N, R and W to  achieve their desired levels of \\nperformance, availability and durability. For instance, the value of \\nN determines the durability of e ach object. A typical value of N \\nused by Dynamo’s users is 3. \\nThe values of W and R impact obj ect availability, durability and \\nconsistency.  For instance, if W is set to 1, then the system will \\nnever reject a write request as long as there is at least one node in \\nthe system that can successfully process a write request. However, \\nlow values of W and R can increase the risk of inconsistency as \\nwrite requests are deemed successful and returned to the clients \\neven if they are not processed by a majority of the replicas. This \\nalso introduces a vulnerability window for durability when a write \\nrequest is successfully returned to the client even though it has \\nbeen persisted at only a small number of nodes.  \\n \\n \\nFigure 4: Average and 99.9 percentiles of latencies for read and \\nwrite requests during our peak re quest season of December 2006. \\nThe intervals between consecutive ticks in the x-axis correspond \\nto 12 hours. Latencies follow a diurnal pattern similar to the \\nrequest rate and 99.9 percentile latencies are an order of \\nmagnitude higher than averages Figure 5: Comparison of perf ormance of 99.9th percentile \\nlatencies for buffered vs. non-buffered writes over a period of \\n24 hours. The intervals between consecutive ticks in the x-axis \\ncorrespond to one hour. \\n204 \\n214 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n Document(id_='8364921d-daaf-4d02-b6d5-b22e0c4ba154', embedding=None, metadata={'page_label': '11', 'file_name': 'amazon-dynamo-sosp2007.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='196761016a037397810d61ca18c463b657779033eef504ba94b5412d37b36143', text='Traditional wisdom holds that dur ability and availability go hand-\\nin-hand. However, this is not necessarily true here. For instance, the vulnerability window for durability can be decreased by \\nincreasing W. This may increase the probability of rejecting \\nrequests (thereby decreasing availability) because more storage \\nhosts need to be alive to process a write request.   \\nThe common (N,R,W) configuration used by several instances of \\nDynamo is (3,2,2). These values are chosen to meet the necessary \\nlevels of performance, durability , consistency, and availability \\nSLAs.  \\nAll the measurements presented in  this section were taken on a \\nlive system operating with a configuration of (3,2,2) and running \\na couple hundred nodes with homogenous hardware \\nconfigurations. As mentioned earl ier, each instance of Dynamo \\ncontains nodes that are located in multiple datacenters. These \\ndatacenters are typically connected through high speed network \\nlinks. Recall that to generate a successful get (or put) response R \\n(or W) nodes need to respond to the coordinator. Clearly, the \\nnetwork latencies between datacen ters affect the response time \\nand the nodes (and their datacenter locations) are chosen such that \\nthe applications target SLAs are met.  \\n6.1 Balancing Performance and Durability  \\nWhile Dynamo’s principle design goal is to build a highly \\navailable data store, performance is an equally important criterion \\nin Amazon’s platform. As noted ear lier, to provide a consistent \\ncustomer experience, Amazon’s services set their performance \\ntargets at higher percentiles (such as the 99.9th or 99.99th \\npercentiles). A typical SLA required of services that use Dynamo \\nis that 99.9% of the read and writ e requests execute  within 300ms.  \\nSince Dynamo is run on standard commodity hardware \\ncomponents that have far le ss I/O throughput than high-end \\nenterprise servers, providing co nsistently high performance for \\nread and write operations is a non- trivial task. The involvement of \\nmultiple storage nodes in read and write operations makes it even more challenging, since the perf ormance of these operations is \\nlimited by the slowest of the R or W replicas. Figure 4 shows the \\naverage and 99.9\\nth percentile latencies of Dynamo’s read and \\nwrite operations during a period of 30 days. As seen in the figure, \\nthe latencies exhibit a clear diurnal pattern which is a result of the \\ndiurnal pattern in the incoming request rate (i.e., there is a significant difference in request rate between the daytime and \\nnight). Moreover, the write latencie s are higher than read latencies \\nobviously because write operations always results in disk access. \\nAlso, the 99.9th percentile latencies are around 200 ms and are an \\norder of magnitude higher than the averages. This is because the \\n99.9th percentile latencies are affected by several factors such as \\nvariability in request load, object sizes, and locality patterns. \\nWhile this level of performance is acceptable for a number of \\nservices, a few customer-facing serv ices required higher levels of \\nperformance. For these services, Dynamo provides the ability to \\ntrade-off durability guarantees for performance. In the \\noptimization each storage node maintains an object buffer in its \\nmain memory. Each write operation is stored in the buffer and \\ngets periodically written to storage by a writer thread . In this \\nscheme, read operations first check if the requested key is present \\nin the buffer. If so, the object is read from the buffer instead of the \\nstorage engine. \\nThis optimization has resulted in lowering the 99.9th percentile \\nlatency by a factor of 5 during p eak traffic even for a very small \\nbuffer of a thousand objects (see Figure 5). Also, as seen in the \\nfigure, write buffering smoothes out  higher percentile latencies. \\nObviously, this scheme trades dura bility for performance. In this \\nscheme, a server crash can result in missing writes that were \\nqueued up in the buffer. To reduce the durability risk, the write \\noperation is refined to have the c oordinator choose one out of the \\nN replicas to perform a “durable write”. Since the coordinator \\nwaits only for W responses, th e performance of the write \\noperation is not affected by the performance of the durable write \\noperation performed by a single replica. \\n6.2 Ensuring Uniform Load distribution \\nDynamo uses consistent hashing to partition its key space across \\nits replicas and to ensure uniform  load distribution. A uniform key \\ndistribution can help us achie ve uniform load distribution \\nassuming the access distribution of keys is not highly skewed. In \\nparticular, Dynamo’s design assumes that even where there is a significant skew in the access distribution there are enough keys \\nin the popular end of the distribution so that the load of handling \\npopular keys can be spread across the nodes uniformly through \\npartitioning. This section discus ses the load imbalance seen in \\nDynamo and the impact of different  partitioning strategies on load \\ndistribution. \\nTo study the load imbalance and its correlation with request load, \\nthe total number of requests receiv ed by each node was measured \\nfor a period of 24 hours - broken down into intervals of 30 \\nminutes. In a given time window, a node is considered to be “in-\\nbalance”, if the node’s request load  deviates from the average load \\nby a value a less than a certain threshold (here 15%). Otherwise \\nthe node was deemed “out-of-bal ance”. Figure 6 presents the \\nfraction of nodes that are “out-of-balance” (henceforth, \\n“imbalance ratio”) during this time period. For reference, the \\ncorresponding request load received  by the entire system during \\nthis time period is also plotte d. As seen in the figure, the \\nimbalance ratio decreases with increasing load. For instance, \\nduring low loads the imbalance ratio is as high as 20% and during \\nhigh loads it is close to 10%. Intuitively, this can be explained by \\nthe fact that under high loads, a large number of popular keys are \\naccessed and due to uniform distribution of keys the load is \\nevenly distributed. However, duri ng low loads (where load is 1/8\\nth \\nFigure 6: Fraction of nodes that  are out-of-balan ce (i.e., nodes \\nwhose request load is above a certain threshold from theaverage system load) and their corresponding request load.\\nThe interval between ticks in x-axis corresponds to a time\\nperiod of 30 minutes. \\n205 \\n215 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n Document(id_='3411f425-2360-4345-90e0-aa31c8b7b27e', embedding=None, metadata={'page_label': '12', 'file_name': 'amazon-dynamo-sosp2007.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='64ed366c34e45c4cfe0c4989bdfc44cdf26d454ab64a334ccad499a7323a0b02', text='of the measured peak load), fewer popular keys are accessed, \\nresulting in a higher load imbalance.  \\nThis section discusses how Dy namo’s partitioning scheme has \\nevolved over time and its implications on load distribution.  \\nStrategy 1: T random tokens per node and partition by token \\nvalue :  This was the initial strategy deployed in production (and \\ndescribed in Section 4.2). In this  scheme, each node is assigned T \\ntokens (chosen uniformly at random from the hash space). The \\ntokens of all nodes are ordered according to their values in the \\nhash space. Every two consecutive tokens define a range. The last \\ntoken and the first token form a range that \"wraps\" around from \\nthe highest value to the lowest value in the hash space. Because \\nthe tokens are chosen randomly, the ranges vary in size. As nodes \\njoin and leave the system, the token set changes and consequently \\nthe ranges change. Note that the space needed to maintain the \\nmembership at each node increases linearly with the number of \\nnodes in the system. \\nWhile using this strategy, the following problems were \\nencountered. First, when a new node joins the system, it needs to \\n“steal” its key ranges from othe r nodes. However, the nodes \\nhanding the key ranges off to the new node have to scan their \\nlocal persistence store to retrieve the appropriate set of data items. \\nNote that performing such a scan operation on a production node \\nis tricky as scans are highly reso urce intensive operations and they \\nneed to be executed in the background without affecting the \\ncustomer performance. This requir es us to run the bootstrapping \\ntask at the lowest priority. However, this significantly slows the \\nbootstrapping process and during busy shopping season, when the \\nnodes are handling millions of requests a day, the bootstrapping \\nhas taken almost a day to complete. Second, when a node \\njoins/leaves the system, the key ranges handled by many nodes \\nchange and the Merkle trees for the new ranges need to be \\nrecalculated, which is a non-tr ivial operation to perform on a \\nproduction system. Finally, there was no easy way to take a \\nsnapshot of the entire key space due to the randomness in key \\nranges, and this made the process of archival complicated. In this \\nscheme, archiving the entire key space requires us to retrieve the \\nkeys from each node separately, which is highly inefficient. The fundamental issue with this st rategy is that the schemes for \\ndata partitioning and data placement are intertwined. For instance, \\nin some cases, it is preferred to  add more nodes to the system in \\norder to handle an increase in request load. However, in this \\nscenario, it is not possible to add nodes without affecting data \\npartitioning. Ideally, it is desira ble to use independent schemes for \\npartitioning and placement. To this  end, following strategies were \\nevaluated:  \\nStrategy 2: T random tokens pe r node and equal sized partitions:  \\nIn this strategy, the hash space is divided into Q equally sized \\npartitions/ranges and each node is  assigned T random tokens. Q is \\nusually set such that Q >> N and Q >> S*T, where S is the \\nnumber of nodes in the system. In this strategy, the tokens are \\nonly used to build the function that  maps values in the hash space \\nto the ordered lists of nodes and not to decide the partitioning. A \\npartition is placed on the first N unique nodes that are encountered \\nwhile walking the consistent hashing ring clockwise from the end \\nof the partition. Figure 7 illustrates this strategy for N=3. In this \\nexample, nodes A, B, C are encountered while walking the ring \\nfrom the end of the partition that  contains key k1. The primary \\nadvantages of this strategy are: (i) decoupling of partitioning and \\npartition placement, and (ii) enabling the possibility of changing \\nthe placement scheme at runtime.   \\nStrategy 3: Q/S tokens per node, equal-sized partitions:  Similar to \\nstrategy 2, this strategy divides the hash space into Q equally \\nsized partitions and the placement of partition is decoupled from \\nthe partitioning scheme. Moreove r, each node is assigned Q/S \\ntokens where S is the number of nodes in the system. When a \\nnode leaves the system, its tokens are randomly distributed to the \\nremaining nodes such that these properties are preserved. \\nSimilarly, when a node joins the system it \"steals\" tokens from \\nnodes in the system in a way that  preserves these properties.  \\nThe efficiency of these three strategies is evaluated for a system \\nwith S=30 and N=3. However,  comparing these different \\nstrategies in a fair manner is ha rd as different strategies have \\ndifferent configurations to tune their efficiency. For instance, the \\nload distribution property of strategy 1 depends on the number of \\ntokens (i.e., T) while strate gy 3 depends on the number of \\npartitions (i.e., Q). One fair way to compare these strategies is to \\n \\nFigure 7: Partitioning and placement of keys in the three strate gies. A, B, and C depict the three unique nodes that form the \\npreference list for the key k1 on the consistent hashing ring (N=3). The shaded area indicates the key range for which nodes A,  \\nB, and C form the preference list. Dark arrows indicate the token locations for various nodes. \\n206 \\n216 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n Document(id_='c91b4481-8221-4f7e-9827-0d2317fc56ca', embedding=None, metadata={'page_label': '13', 'file_name': 'amazon-dynamo-sosp2007.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='a6babde156fb0b8dc7df6d3599c3e3a9e5f246e46c14a824bf72fd015dbd7e32', text='evaluate the skew in their load distribution while all strategies use \\nthe same amount of space to maintain their membership \\ninformation. For instance, in strategy 1 each node needs to \\nmaintain the token positions of all the nodes in the ring and in \\nstrategy 3 each node needs to maintain the information regarding \\nthe partitions assigned to each node. \\nIn our next experiment, these strategies were evaluated by varying \\nthe relevant parameters (T and Q) . The load balancing efficiency \\nof each strategy was measured for different sizes of membership \\ninformation that needs to be maintained at each node, where Load \\nbalancing efficiency  is defined as the ratio of average number of \\nrequests served by each node to the maximum number of requests \\nserved by the hottest node. \\nThe results are given in Figure 8. As seen in the figure, strategy 3 \\nachieves the best load balancing e fficiency and strategy 2 has the \\nworst load balancing efficiency. For a brief time, Strategy 2 \\nserved as an interim setup du ring the process of migrating \\nDynamo instances from using Strate gy 1 to Strategy 3. Compared \\nto Strategy 1, Strategy 3 achieves better efficiency and reduces the \\nsize of membership information ma intained at each node by three \\norders of magnitude. While storag e is not a major issue the nodes \\ngossip the membership information periodically and as such it is \\ndesirable to keep this information as compact as possible.  In \\naddition to this, strategy 3 is advantageous and simpler to deploy \\nfor the following reasons: (i) Faster bootstrapping/recovery:  \\nSince partition ranges are fixed, they can be stored in separate \\nfiles, meaning a partition can be relocated as a unit by simply \\ntransferring the file (avoiding random accesses needed to locate \\nspecific items). This simplifies the process of bootstrapping and \\nrecovery. (ii) Ease of archival : Periodical archivi ng of the dataset \\nis a mandatory requirement for mo st of Amazon storage services. \\nArchiving the entire dataset stored by Dynamo is simpler in \\nstrategy 3 because the partition files can be archived separately. \\nBy contrast, in Strategy 1, the tokens are chosen randomly and,  \\narchiving the data stored in Dynamo requires retrieving the keys \\nfrom individual nodes sepa rately and is usually inefficient and \\nslow. The disadvantage of strategy 3 is that changing the node \\nmembership requires coordination in order to preserve the \\nproperties required of the assignment.  6.3 Divergent Versions: When and  \\nHow Many? \\nAs noted earlier, Dynamo is desi gned to tradeoff consistency for \\navailability. To understa nd the precise impact of different failures \\non consistency, detailed data is required on multiple factors: \\noutage length, type of failure, com ponent reliability, workload etc. \\nPresenting these numbers in detail is  outside of the scope of this \\npaper. However, this section discusses a good summary metric:  \\nthe number of divergent versions s een by the application in a live \\nproduction environment.  \\nDivergent versions of a data item arise in two scenarios. The first \\nis when the system is facing failure scenarios such as node \\nfailures, data center failures, and network partitions. The second is \\nwhen the system is handling a la rge number of concurrent writers \\nto a single data item and multiple nodes end up coordinating the \\nupdates concurrently. From both a usability and efficiency \\nperspective, it is preferred to  keep the number of divergent \\nversions at any given time as lo w as possible. If the versions \\ncannot be syntactically reconciled  based on vector clocks alone, \\nthey have to be passed to th e business logic for semantic \\nreconciliation. Semantic reconciliation introduces additional load \\non services, so it is desirable to minimize the need for it.  \\nIn our next experiment, the number of versions returned to the \\nshopping cart service was profiled for a period of 24 hours.  \\nDuring this period, 99.94% of re quests saw exactly one version; \\n0.00057% of requests saw 2 vers ions; 0.00047% of requests saw 3 \\nversions and 0.00009% of requests  saw 4 versions. This shows \\nthat divergent versions are created rarely.  \\nExperience shows that the increase in the number of divergent \\nversions is contributed not by failures but due to the increase in \\nnumber of concurrent writers. The increase in the number of \\nconcurrent writes is usually tr iggered by busy robots (automated \\nclient programs) and rarely by hu mans. This issue is not discussed \\nin detail due to the sensitive nature of the story.  \\n6.4 Client-driven or Server-driven \\nCoordination \\nAs mentioned in Section 5, Dy namo has a request coordination \\ncomponent that uses a state mach ine to handle incoming requests. \\nClient requests are uniformly as signed to nodes in the ring by a \\nload balancer. Any Dynamo node can act as a coordinator for a \\nread request. Write requests on th e other hand will be coordinated \\nby a node in the key’s current preference list. This restriction is \\ndue to the fact that these preferred nodes have the added \\nresponsibility of creating a new version stamp that causally \\nsubsumes the version that has be en updated by the write request. \\nNote that if Dynamo’s versioni ng scheme is based on physical \\ntimestamps, any node can coor dinate a write request. \\nAn alternative approach to request coordination is to move the \\nstate machine to the client node s. In this scheme client \\napplications use a library to perfo rm request coordination locally. \\nA client periodically pick s a random Dynamo node and \\ndownloads its current view of Dy namo membership state. Using \\nthis information the client can determine which set of nodes form \\nthe preference list for any give n key. Read requests can be \\ncoordinated at the client node thereby avoiding the extra network \\nhop that is incurred if the request were assigned to a random Dynamo node by the load balancer. Writes will either be \\nforwarded to a node in the key’s preference list or can be 0.40.50.60.70.80.91\\n0 5000 10000 15000 20000 25000 30000 35000\\nSize of metadata maintained at each node (in abstract units)Efficieny (mean load/max load)Strategy 1\\nStrategy 2\\nStrategy 3\\nFigure 8: Comparison of the l oad distribution efficiency o f\\ndifferent strategies for system with 30 nodes and N=3 with\\nequal amount of metadata maintained at each node. The\\nvalues of the system size and number of replicas are based on\\nthe typical configuration deployed for majority of our\\nservices. \\n207 \\n217 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n Document(id_='ed3e84fb-2d22-49c8-a453-4cc2972d60ef', embedding=None, metadata={'page_label': '14', 'file_name': 'amazon-dynamo-sosp2007.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='a3905f9cc8ab6b79c7991c4cc9048ab9ec9ecc01ba27a71ab65160a93c285456', text='coordinated locally if Dynamo  is using timestamps based \\nversioning.  \\nAn important advantage of th e client-driven coordination \\napproach is that a load balancer is no longer required to uniformly \\ndistribute client load. Fair load distribution is implicitly \\nguaranteed by the near uniform a ssignment of keys to the storage \\nnodes. Obviously, the efficiency of  this scheme is dependent on \\nhow fresh the membership information is at the client. Currently \\nclients poll a random Dynamo node every 10 seconds for \\nmembership updates. A pull base d approach was chosen over a \\npush based one as the former scal es better with large number of \\nclients and requires very little state to be maintained at servers \\nregarding clients. However, in the worst case the client can be \\nexposed to stale membership for duration of 10 seconds. In case, \\nif the client detects its membership table is stale (for instance, \\nwhen some members are unreachable), it will immediately refresh \\nits membership information.  \\nTable 2 shows the latency improvements at the 99.9th percentile \\nand averages that were observed for a period of 24 hours using \\nclient-driven coordination compared to the server-driven \\napproach. As seen in the table,  the client-driven coordination \\napproach reduces the latencies by at least 30 milliseconds for \\n99.9th percentile latencies and decreases the average by 3 to 4 \\nmilliseconds. The latency improvement is because the client-\\ndriven approach eliminates the overhead of the load balancer and \\nthe extra network hop that may be incurred when a request is \\nassigned to a random node. As seen in the table, average latencies \\ntend to be significantly lower than latencies at the 99.9th \\npercentile. This is because Dynamo’s storage engine caches and \\nwrite buffer have good hit ratios. Moreover, since the load \\nbalancers and network introduce additional variability to the \\nresponse time, the gain in res ponse time is higher for the 99.9th \\npercentile than the average.  \\n6.5 Balancing background vs. foreground \\ntasks \\nEach node performs different kinds of background tasks for \\nreplica synchronization and data handoff (either due to hinting or \\nadding/removing nodes) in addition to its normal foreground \\nput/get operations. In early prod uction settings, these background \\ntasks triggered the problem of resource contention and affected \\nthe performance of the regular put  and get operations. Hence, it \\nbecame necessary to ensure that background tasks ran only when \\nthe regular critical operations ar e not affected significantly. To \\nthis end, the background tasks were integrated with an admission \\ncontrol mechanism. Each of the background tasks uses this \\ncontroller to reserve runtime slices of the resource (e.g. database), shared across all background tasks.  A feedback mechanism based \\non the monitored performance of the foreground tasks is \\nemployed to change the number of slices that are available to the \\nbackground tasks. \\nThe admission controller constantly monitors the behavior of \\nresource accesses while execu ting a \"foreground\" put/get \\noperation. Monitored aspects include  latencies for disk operations, \\nfailed database accesses due to lock-contention and transaction \\ntimeouts, and request que ue wait times. This information is used \\nto check whether the percentiles of latencies (or failures) in a \\ngiven trailing time window are clos e to a desired threshold. For \\nexample, the background controller checks to see how close the \\n99th percentile database read late ncy (over the last 60 seconds) is \\nto a preset threshold (say 50ms ).  The controller uses such \\ncomparisons to assess the resource availability for the foreground \\noperations. Subsequently, it decide s on how many time slices will \\nbe available to background tasks, thereby using the feedback loop \\nto limit the intrusiveness of the background activities.  Note that a \\nsimilar problem of managing ba ckground tasks has been studied \\nin [4]. \\n6.6 Discussion \\nThis section summarizes some of the experiences gained during \\nthe process of implementation and maintenance of Dynamo. \\nMany Amazon internal services ha ve used Dynamo for the past \\ntwo years and it has provided signifi cant levels of availability to \\nits applications. In particular , applications have received \\nsuccessful responses (without timing out) for 99.9995% of its \\nrequests and no data loss event has occurred to date.  \\nMoreover, the primary advantage of Dynamo is that it provides \\nthe necessary knobs using the three parameters of (N,R,W) to tune \\ntheir instance based on their needs.. Unlike popular commercial \\ndata stores, Dynamo exposes data  consistency and reconciliation \\nlogic issues to the developers. At  the outset, one may expect the \\napplication logic to become more complex. However, historically, \\nAmazon’s platform is built for high availability and many applications are designed to ha ndle different failure modes and \\ninconsistencies that may arise. He nce, porting such applications to \\nuse Dynamo was a relatively simple task. For new applications \\nthat want to use Dynamo, some analysis is required during the \\ninitial stages of the development to pick the right conflict \\nresolution mechanisms that meet the business case appropriately. \\nFinally, Dynamo adopts a full membership model where each \\nnode is aware of the data hosted by its peers. To do this, each \\nnode actively gossips the full routi ng table with other nodes in the \\nsystem. This model works well for a system that contains couple \\nof hundreds of nodes. However, s caling such a design to run with \\ntens of thousands of nodes is not trivial because the overhead in \\nmaintaining the routing table increas es with the system size. This \\nlimitation might be overcome by  introducing hierarchical \\nextensions to Dynamo. Also, note  that this problem is actively \\naddressed by O(1) DHT systems(e.g., [14]). \\n7. CONCLUSIONS \\nThis paper described Dynamo, a highly available and scalable \\ndata store, used for storing state of a number of core services of \\nAmazon.com’s e-commerce platfo rm. Dynamo has provided the \\ndesired levels of availability  and performance and has been \\nsuccessful in handling server failu res, data center failures and \\nnetwork partitions. Dynamo is incrementally scalable and allows \\nservice owners to scale up and down based on their current Table 2: Performance of client-driven and server-driven coordination approaches. \\n 99.9th \\npercentile \\nread \\nlatency \\n(ms) 99.9th \\npercentile \\nwrite \\nlatency \\n(ms) Average \\nread \\nlatency \\n(ms) Average \\nwrite \\nlatency \\n(ms) \\nServer-\\ndriven 68.9 68.5 3.9 4.02 \\nClient-\\ndriven 30.4 30.4 1.55 1.9 \\n \\n208 \\n218 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n Document(id_='54e6ac5f-5f68-4d55-9345-977732fd14c7', embedding=None, metadata={'page_label': '15', 'file_name': 'amazon-dynamo-sosp2007.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='371b54d3c55c5a7efc0de019a6c2772266efb857972dc4beca89cfbbfc674310', text=\"request load. Dynamo allows serv ice owners to customize their \\nstorage system to meet their desired performance, durability and \\nconsistency SLAs by allowing them to tune the parameters N, R, \\nand W. \\nThe production use of Dynamo for the past year demonstrates that \\ndecentralized techniques can be combined to provide a single \\nhighly-available system. Its success in one of the most \\nchallenging application environments shows that an eventual-\\nconsistent storage system can be a building block for highly-\\navailable applications.  \\nACKNOWLEDGEMENTS \\nThe authors would like to thank Pat Helland for his contribution to the initial design of Dynamo. We would also like to thank \\nMarvin Theimer and Robert va n Renesse for their comments. \\nFinally, we would like to thank our shepherd, Jeff Mogul, for his \\ndetailed comments and inputs while  preparing the camera ready \\nversion that vastly improve d the quality of the paper. \\nREFERENCES \\n[1] Adya, A., Bolosky, W. J., Castro, M., Cermak, G., Chaiken, \\nR., Douceur, J. R., Howell, J., Lorch, J. R., Theimer, M., and \\nWattenhofer, R. P. 2002. Farsite: federated, available, and \\nreliable storage for an incompletely trusted environment. \\nSIGOPS Oper. Syst. Rev.  36, SI (Dec. 2002), 1-14.  \\n[2]  Bernstein, P.A., and G oodman, N. An algorithm for \\nconcurrency control and recovery in replicated distributed \\ndatabases. ACM Trans. on Database Systems, 9(4):596-615, \\nDecember 1984 \\n[3] Chang, F., Dean, J., Ghemawat, S., Hsieh, W. C., Wallach, \\nD. A., Burrows, M., Chandra, T., Fikes, A., and Gruber, R. \\nE. 2006. Bigtable: a distributed storage system for structured \\ndata. In Proceedings of the 7th Conference on USENIX \\nSymposium on Operating Systems Design and \\nImplementation - Volume 7  (Seattle, WA, November 06 - 08, \\n2006). USENIX Association, Berkeley, CA, 15-15. \\n[4] Douceur, J. R. and Bolosky, W. J. 2000. Process-based \\nregulation of low-importance processes. SIGOPS Oper. Syst. \\nRev. 34, 2 (Apr. 2000), 26-27.  \\n[5] Fox, A., Gribble, S. D., Chawathe, Y., Brewer, E. A., and \\nGauthier, P. 1997. Cluster-based  scalable network services. \\nIn Proceedings of the Sixt eenth ACM Symposium on \\nOperating Systems Principles  (Saint Malo, France, October \\n05 - 08, 1997). W. M. Waite, Ed. SOSP '97. ACM Press, \\nNew York, NY, 78-91.  \\n[6] Ghemawat, S., Gobioff, H., and Leung, S. 2003. The Google \\nfile system. In Proceedings of the Nineteenth ACM \\nSymposium on Operating Systems Principles  (Bolton \\nLanding, NY, USA, October 19 - 22, 2003). SOSP '03. ACM \\nPress, New York, NY, 29-43.  \\n[7] Gray, J., Helland, P., O'Neil, P., and Shasha, D. 1996. The \\ndangers of replication and a solution. In Proceedings of the \\n1996 ACM SIGMOD international Conference on \\nManagement of Data  (Montreal, Quebec, Canada, June 04 - \\n06, 1996). J. Widom, Ed. SIGMOD '96. ACM Press, New \\nYork, NY, 173-182.  \\n[8] Gupta, I., Chandra, T. D., and Goldszmidt, G. S. 2001. On \\nscalable and efficient distributed failure detectors. In \\nProceedings of the Twentieth Annual ACM Symposium on Principles of Dist ributed Computing  (Newport, Rhode \\nIsland, United States). PODC  '01. ACM Press, New York, \\nNY, 170-179.  \\n[9] Kubiatowicz, J., Bindel, D., Ch en, Y., Czerwinski, S., Eaton, \\nP., Geels, D., Gummadi, R., Rhea, S., Weatherspoon, H., \\nWells, C., and Zhao, B. 2000. OceanStore: an architecture \\nfor global-scale persistent storage. SIGARCH Comput. \\nArchit. News  28, 5 (Dec. 2000), 190-201.  \\n[10] Karger, D., Lehman, E., Leighton, T., Panigrahy, R., Levine, \\nM., and Lewin, D. 1997. Cons istent hashing and random \\ntrees: distributed caching protocols for relieving hot spots on \\nthe World Wide Web. In Proceedings of the Twenty-Ninth \\nAnnual ACM Symposium on theory of Computing  (El Paso, \\nTexas, United States, May 04 - 06, 1997). STOC '97. ACM \\nPress, New York, NY, 654-663.  \\n[11]  Lindsay, B.G.,  et. al., “Not es on Distributed Databases”, \\nResearch Report RJ2571(33471), IBM Research, July 1979 \\n[12]  Lamport, L. Time, clocks, an d the ordering of events in a \\ndistributed system. ACM Communications, 21(7), pp. 558-\\n565, 1978. \\n[13] Merkle, R. A digital signatu re based on a conventional \\nencryption function. Proceedings of CRYPTO, pages 369–\\n378. Springer-Verlag, 1988. \\n[14] Ramasubramanian, V., and Sirer,  E. G.  Beehive: O(1)lookup \\nperformance for power-law quer y distributions in peer-to-\\npeer overlays. In Proceedings of the 1st Conference on \\nSymposium on Networked Systems Design and \\nImplementation, San Francisco, CA, March 29 - 31, 2004.  \\n[15] Reiher, P., Heidemann, J., Ratner, D., Skinner, G., and \\nPopek, G. 1994. Resolving file conflicts in the Ficus file \\nsystem. In Proceedings of the USENIX Summer 1994 \\nTechnical Conference on USENIX Summer 1994 Technical \\nConference - Volume 1  (Boston, Massachusetts, June 06 - 10, \\n1994). USENIX Association, Berkeley, CA, 12-12.. \\n[16] Rowstron, A., and Druschel, P. Pastry: Scalable, \\ndecentralized object location and routing for large-scale peer-\\nto-peer systems. Proceedings of Middleware, pages 329-350, \\nNovember, 2001. \\n[17]  Rowstron, A.,  and Druschel , P. Storage management and \\ncaching in PAST, a large-scale, persistent peer-to-peer \\nstorage utility. Proceedings of Symposium on Operating \\nSystems Principles, October 2001. \\n[18]  Saito, Y., Frølund, S., Veitch,  A., Merchant, A., and Spence, \\nS. 2004. FAB: building distributed enterprise disk arrays \\nfrom commodity components. SIGOPS Oper. Syst. Rev.  38, 5 \\n(Dec. 2004), 48-58.  \\n[19] Satyanarayanan, M., Kistler, J.J., Siegel, E.H. Coda: A \\nResilient Distributed File System. IEEE Workshop on \\nWorkstation Operating Systems, Nov. 1987. \\n[20] Stoica, I., Morris, R., Karger, D., Kaashoek, M. F., and \\nBalakrishnan, H. 2001. Chord:  A scalable peer-to-peer \\nlookup service for internet applications. In Proceedings of \\nthe 2001 Conference on Appl ications, Technologies, \\nArchitectures, and Protocols For Computer Communications  \\n(San Diego, California, United States). SIGCOMM '01. \\nACM Press, New York, NY, 149-160.  \\n209 \\n219 \", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n Document(id_='f7af69f5-fbb2-4dec-b4f4-e355f63c88b3', embedding=None, metadata={'page_label': '16', 'file_name': 'amazon-dynamo-sosp2007.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='35e0a12aa704178bddee530efd76c8e29220da6f4ea33f9c914eb5fb11579d8f', text=\"[21] Terry, D. B., Theimer, M. M., Petersen, K., Demers, A. J., \\nSpreitzer, M. J., and Hauser , C. H. 1995. Managing update \\nconflicts in Bayou, a weakly connected replicated storage \\nsystem. In Proceedings of the Fift eenth ACM Symposium on \\nOperating Systems Principles  (Copper Mountain, Colorado, \\nUnited States, December 03 - 06, 1995). M. B. Jones, Ed. \\nSOSP '95. ACM Press, New York, NY, 172-182.  \\n[22]  Thomas, R. H.  A majority consensus approach to \\nconcurrency control for multiple copy databases. ACM \\nTransactions on Database Systems 4 (2): 180-209, 1979. [23]  Weatherspoon, H., Eaton, P., Ch un, B., and Kubi atowicz, J. \\n2007. Antiquity: exploiting a secure log for wide-area \\ndistributed storage. SIGOPS Oper. Syst. Rev.  41, 3 (Jun. \\n2007), 371-384.  \\n[24] Welsh, M., Culler, D., and Brewer, E. 2001. SEDA: an \\narchitecture for well-conditioned, scalable internet services. \\nIn Proceedings of the Eighteenth ACM Symposium on \\nOperating Systems Principles  (Banff, Alberta, Canada, \\nOctober 21 - 24, 2001). SOSP '01. ACM Press, New York, \\nNY, 230-243.  \\n \\n \\n210 \\n220 \", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "import openai\n",
    "\n",
    "\n",
    "from llama_index.readers.github_readers.github_api_client import GithubClient\n",
    "from llama_index import download_loader, GithubRepositoryReader\n",
    "# UPDATE: Since llma_index changed their library, the following code should replace the code above\n",
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader\n",
    "import os\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = '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'\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "# os.environ['GITHUB_TOKEN']  = \"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\"\n",
    "#\n",
    "# # download_loader(\"GithubRepositoryReader\")\n",
    "#\n",
    "# documents = None\n",
    "# if os.path.exists(\"docs.pkl\"):\n",
    "#     with open(\"docs.pkl\", \"rb\") as f:\n",
    "#         documents = pickle.load(f)\n",
    "#\n",
    "# if documents is None:\n",
    "#     github_client = GithubClient(os.getenv(\"GITHUB_TOKEN\"))\n",
    "#     loader = GithubRepositoryReader(\n",
    "#         github_client,\n",
    "#         repo =                   \"RidePlanner\",\n",
    "#         verbose =                True,\n",
    "#          concurrent_requests =    10,\n",
    "#     )\n",
    "#\n",
    "#     documents = loader.load_data(branch=\"main\")\n",
    "#\n",
    "#     with open(\"docs.pkl\", \"wb\") as f:\n",
    "#         pickle.dump(documents, f)\n",
    "\n",
    "\n",
    "PDFReader = download_loader(\"PDFReader\")\n",
    "\n",
    "loader = PDFReader()\n",
    "documents = loader.load_data(file=Path('./amazon-dynamo-sosp2007.pdf'))\n",
    "documents"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-29T18:42:15.000543Z",
     "start_time": "2023-06-29T18:42:14.739308Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "ename": "RetryError",
     "evalue": "RetryError[<Future at 0x17243faf0 state=finished raised RateLimitError>]",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRateLimitError\u001B[0m                            Traceback (most recent call last)",
      "File \u001B[0;32m~/Documents/Personal Projects/test_demo_for_llm/venv/lib/python3.9/site-packages/tenacity/__init__.py:382\u001B[0m, in \u001B[0;36mRetrying.__call__\u001B[0;34m(self, fn, *args, **kwargs)\u001B[0m\n\u001B[1;32m    381\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 382\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    383\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m:  \u001B[38;5;66;03m# noqa: B902\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/Personal Projects/test_demo_for_llm/venv/lib/python3.9/site-packages/llama_index/embeddings/openai.py:150\u001B[0m, in \u001B[0;36mget_embeddings\u001B[0;34m(list_of_text, engine, **kwargs)\u001B[0m\n\u001B[1;32m    148\u001B[0m list_of_text \u001B[38;5;241m=\u001B[39m [text\u001B[38;5;241m.\u001B[39mreplace(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m text \u001B[38;5;129;01min\u001B[39;00m list_of_text]\n\u001B[0;32m--> 150\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[43mopenai\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mEmbedding\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlist_of_text\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mengine\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mdata\n\u001B[1;32m    151\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m [d[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124membedding\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m d \u001B[38;5;129;01min\u001B[39;00m data]\n",
      "File \u001B[0;32m~/Documents/Personal Projects/test_demo_for_llm/venv/lib/python3.9/site-packages/openai/api_resources/embedding.py:33\u001B[0m, in \u001B[0;36mEmbedding.create\u001B[0;34m(cls, *args, **kwargs)\u001B[0m\n\u001B[1;32m     32\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 33\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     35\u001B[0m     \u001B[38;5;66;03m# If a user specifies base64, we'll just return the encoded string.\u001B[39;00m\n\u001B[1;32m     36\u001B[0m     \u001B[38;5;66;03m# This is only for the default case.\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/Personal Projects/test_demo_for_llm/venv/lib/python3.9/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001B[0m, in \u001B[0;36mEngineAPIResource.create\u001B[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001B[0m\n\u001B[1;32m    138\u001B[0m (\n\u001B[1;32m    139\u001B[0m     deployment_id,\n\u001B[1;32m    140\u001B[0m     engine,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    150\u001B[0m     api_key, api_base, api_type, api_version, organization, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams\n\u001B[1;32m    151\u001B[0m )\n\u001B[0;32m--> 153\u001B[0m response, _, api_key \u001B[38;5;241m=\u001B[39m \u001B[43mrequestor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    154\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpost\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    155\u001B[0m \u001B[43m    \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    156\u001B[0m \u001B[43m    \u001B[49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    157\u001B[0m \u001B[43m    \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    158\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    159\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrequest_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    160\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrequest_timeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest_timeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    161\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    163\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m stream:\n\u001B[1;32m    164\u001B[0m     \u001B[38;5;66;03m# must be an iterator\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/Personal Projects/test_demo_for_llm/venv/lib/python3.9/site-packages/openai/api_requestor.py:298\u001B[0m, in \u001B[0;36mAPIRequestor.request\u001B[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001B[0m\n\u001B[1;32m    288\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrequest_raw(\n\u001B[1;32m    289\u001B[0m     method\u001B[38;5;241m.\u001B[39mlower(),\n\u001B[1;32m    290\u001B[0m     url,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    296\u001B[0m     request_timeout\u001B[38;5;241m=\u001B[39mrequest_timeout,\n\u001B[1;32m    297\u001B[0m )\n\u001B[0;32m--> 298\u001B[0m resp, got_stream \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_interpret_response\u001B[49m\u001B[43m(\u001B[49m\u001B[43mresult\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    299\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m resp, got_stream, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapi_key\n",
      "File \u001B[0;32m~/Documents/Personal Projects/test_demo_for_llm/venv/lib/python3.9/site-packages/openai/api_requestor.py:700\u001B[0m, in \u001B[0;36mAPIRequestor._interpret_response\u001B[0;34m(self, result, stream)\u001B[0m\n\u001B[1;32m    698\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    699\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (\n\u001B[0;32m--> 700\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_interpret_response_line\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    701\u001B[0m \u001B[43m            \u001B[49m\u001B[43mresult\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcontent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mutf-8\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    702\u001B[0m \u001B[43m            \u001B[49m\u001B[43mresult\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstatus_code\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    703\u001B[0m \u001B[43m            \u001B[49m\u001B[43mresult\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    704\u001B[0m \u001B[43m            \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    705\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m,\n\u001B[1;32m    706\u001B[0m         \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    707\u001B[0m     )\n",
      "File \u001B[0;32m~/Documents/Personal Projects/test_demo_for_llm/venv/lib/python3.9/site-packages/openai/api_requestor.py:763\u001B[0m, in \u001B[0;36mAPIRequestor._interpret_response_line\u001B[0;34m(self, rbody, rcode, rheaders, stream)\u001B[0m\n\u001B[1;32m    762\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m stream_error \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;241m200\u001B[39m \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m rcode \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m300\u001B[39m:\n\u001B[0;32m--> 763\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandle_error_response(\n\u001B[1;32m    764\u001B[0m         rbody, rcode, resp\u001B[38;5;241m.\u001B[39mdata, rheaders, stream_error\u001B[38;5;241m=\u001B[39mstream_error\n\u001B[1;32m    765\u001B[0m     )\n\u001B[1;32m    766\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m resp\n",
      "\u001B[0;31mRateLimitError\u001B[0m: You exceeded your current quota, please check your plan and billing details.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mRetryError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[25], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# UPDATE: Since llma_index changed their library, the following code should replace the code above\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m index \u001B[38;5;241m=\u001B[39m \u001B[43mVectorStoreIndex\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_documents\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdocuments\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/Personal Projects/test_demo_for_llm/venv/lib/python3.9/site-packages/llama_index/indices/base.py:96\u001B[0m, in \u001B[0;36mBaseIndex.from_documents\u001B[0;34m(cls, documents, storage_context, service_context, **kwargs)\u001B[0m\n\u001B[1;32m     93\u001B[0m     docstore\u001B[38;5;241m.\u001B[39mset_document_hash(doc\u001B[38;5;241m.\u001B[39mget_doc_id(), doc\u001B[38;5;241m.\u001B[39mhash)\n\u001B[1;32m     94\u001B[0m nodes \u001B[38;5;241m=\u001B[39m service_context\u001B[38;5;241m.\u001B[39mnode_parser\u001B[38;5;241m.\u001B[39mget_nodes_from_documents(documents)\n\u001B[0;32m---> 96\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[1;32m     97\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnodes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnodes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     98\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_context\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstorage_context\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     99\u001B[0m \u001B[43m    \u001B[49m\u001B[43mservice_context\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mservice_context\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    100\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    101\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/Personal Projects/test_demo_for_llm/venv/lib/python3.9/site-packages/llama_index/indices/vector_store/base.py:45\u001B[0m, in \u001B[0;36mVectorStoreIndex.__init__\u001B[0;34m(self, nodes, index_struct, service_context, storage_context, use_async, store_nodes_override, **kwargs)\u001B[0m\n\u001B[1;32m     43\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_use_async \u001B[38;5;241m=\u001B[39m use_async\n\u001B[1;32m     44\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_store_nodes_override \u001B[38;5;241m=\u001B[39m store_nodes_override\n\u001B[0;32m---> 45\u001B[0m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[1;32m     46\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnodes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnodes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     47\u001B[0m \u001B[43m    \u001B[49m\u001B[43mindex_struct\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mindex_struct\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     48\u001B[0m \u001B[43m    \u001B[49m\u001B[43mservice_context\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mservice_context\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     49\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_context\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstorage_context\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     50\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     51\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/Personal Projects/test_demo_for_llm/venv/lib/python3.9/site-packages/llama_index/indices/base.py:68\u001B[0m, in \u001B[0;36mBaseIndex.__init__\u001B[0;34m(self, nodes, index_struct, storage_context, service_context, **kwargs)\u001B[0m\n\u001B[1;32m     66\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m index_struct \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     67\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m nodes \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m---> 68\u001B[0m     index_struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbuild_index_from_nodes\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnodes\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     69\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_index_struct \u001B[38;5;241m=\u001B[39m index_struct\n\u001B[1;32m     70\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_storage_context\u001B[38;5;241m.\u001B[39mindex_store\u001B[38;5;241m.\u001B[39madd_index_struct(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_index_struct)\n",
      "File \u001B[0;32m~/Documents/Personal Projects/test_demo_for_llm/venv/lib/python3.9/site-packages/llama_index/token_counter/token_counter.py:78\u001B[0m, in \u001B[0;36mllm_token_counter.<locals>.wrap.<locals>.wrapped_llm_predict\u001B[0;34m(_self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     76\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapped_llm_predict\u001B[39m(_self: Any, \u001B[38;5;241m*\u001B[39margs: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[1;32m     77\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m wrapper_logic(_self):\n\u001B[0;32m---> 78\u001B[0m         f_return_val \u001B[38;5;241m=\u001B[39m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_self\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     80\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m f_return_val\n",
      "File \u001B[0;32m~/Documents/Personal Projects/test_demo_for_llm/venv/lib/python3.9/site-packages/llama_index/indices/vector_store/base.py:225\u001B[0m, in \u001B[0;36mVectorStoreIndex.build_index_from_nodes\u001B[0;34m(self, nodes)\u001B[0m\n\u001B[1;32m    217\u001B[0m \u001B[38;5;129m@llm_token_counter\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbuild_index_from_nodes\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    218\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mbuild_index_from_nodes\u001B[39m(\u001B[38;5;28mself\u001B[39m, nodes: Sequence[BaseNode]) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m IndexDict:\n\u001B[1;32m    219\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Build the index from nodes.\u001B[39;00m\n\u001B[1;32m    220\u001B[0m \n\u001B[1;32m    221\u001B[0m \u001B[38;5;124;03m    NOTE: Overrides BaseIndex.build_index_from_nodes.\u001B[39;00m\n\u001B[1;32m    222\u001B[0m \u001B[38;5;124;03m        VectorStoreIndex only stores nodes in document store\u001B[39;00m\n\u001B[1;32m    223\u001B[0m \u001B[38;5;124;03m        if vector store does not store text\u001B[39;00m\n\u001B[1;32m    224\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 225\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_build_index_from_nodes\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnodes\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/Personal Projects/test_demo_for_llm/venv/lib/python3.9/site-packages/llama_index/indices/vector_store/base.py:214\u001B[0m, in \u001B[0;36mVectorStoreIndex._build_index_from_nodes\u001B[0;34m(self, nodes)\u001B[0m\n\u001B[1;32m    212\u001B[0m     run_async_tasks(tasks)\n\u001B[1;32m    213\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 214\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_add_nodes_to_index\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex_struct\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnodes\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    215\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m index_struct\n",
      "File \u001B[0;32m~/Documents/Personal Projects/test_demo_for_llm/venv/lib/python3.9/site-packages/llama_index/indices/vector_store/base.py:190\u001B[0m, in \u001B[0;36mVectorStoreIndex._add_nodes_to_index\u001B[0;34m(self, index_struct, nodes)\u001B[0m\n\u001B[1;32m    187\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m nodes:\n\u001B[1;32m    188\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[0;32m--> 190\u001B[0m embedding_results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_node_embedding_results\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnodes\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    191\u001B[0m new_ids \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_vector_store\u001B[38;5;241m.\u001B[39madd(embedding_results)\n\u001B[1;32m    193\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_vector_store\u001B[38;5;241m.\u001B[39mstores_text \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_store_nodes_override:\n\u001B[1;32m    194\u001B[0m     \u001B[38;5;66;03m# NOTE: if the vector store doesn't store text,\u001B[39;00m\n\u001B[1;32m    195\u001B[0m     \u001B[38;5;66;03m# we need to add the nodes to the index struct and document store\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/Personal Projects/test_demo_for_llm/venv/lib/python3.9/site-packages/llama_index/indices/vector_store/base.py:107\u001B[0m, in \u001B[0;36mVectorStoreIndex._get_node_embedding_results\u001B[0;34m(self, nodes)\u001B[0m\n\u001B[1;32m    101\u001B[0m         id_to_embed_map[n\u001B[38;5;241m.\u001B[39mnode_id] \u001B[38;5;241m=\u001B[39m n\u001B[38;5;241m.\u001B[39membedding\n\u001B[1;32m    103\u001B[0m \u001B[38;5;66;03m# call embedding model to get embeddings\u001B[39;00m\n\u001B[1;32m    104\u001B[0m (\n\u001B[1;32m    105\u001B[0m     result_ids,\n\u001B[1;32m    106\u001B[0m     result_embeddings,\n\u001B[0;32m--> 107\u001B[0m ) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_service_context\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membed_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_queued_text_embeddings\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    108\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m new_id, text_embedding \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(result_ids, result_embeddings):\n\u001B[1;32m    109\u001B[0m     id_to_embed_map[new_id] \u001B[38;5;241m=\u001B[39m text_embedding\n",
      "File \u001B[0;32m~/Documents/Personal Projects/test_demo_for_llm/venv/lib/python3.9/site-packages/llama_index/embeddings/base.py:173\u001B[0m, in \u001B[0;36mBaseEmbedding.get_queued_text_embeddings\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    171\u001B[0m cur_batch_ids \u001B[38;5;241m=\u001B[39m [text_id \u001B[38;5;28;01mfor\u001B[39;00m text_id, _ \u001B[38;5;129;01min\u001B[39;00m cur_batch]\n\u001B[1;32m    172\u001B[0m cur_batch_texts \u001B[38;5;241m=\u001B[39m [text \u001B[38;5;28;01mfor\u001B[39;00m _, text \u001B[38;5;129;01min\u001B[39;00m cur_batch]\n\u001B[0;32m--> 173\u001B[0m embeddings \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_text_embeddings\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcur_batch_texts\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    174\u001B[0m result_ids\u001B[38;5;241m.\u001B[39mextend(cur_batch_ids)\n\u001B[1;32m    175\u001B[0m result_embeddings\u001B[38;5;241m.\u001B[39mextend(embeddings)\n",
      "File \u001B[0;32m~/Documents/Personal Projects/test_demo_for_llm/venv/lib/python3.9/site-packages/llama_index/embeddings/openai.py:267\u001B[0m, in \u001B[0;36mOpenAIEmbedding._get_text_embeddings\u001B[0;34m(self, texts)\u001B[0m\n\u001B[1;32m    260\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_get_text_embeddings\u001B[39m(\u001B[38;5;28mself\u001B[39m, texts: List[\u001B[38;5;28mstr\u001B[39m]) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m List[List[\u001B[38;5;28mfloat\u001B[39m]]:\n\u001B[1;32m    261\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Get text embeddings.\u001B[39;00m\n\u001B[1;32m    262\u001B[0m \n\u001B[1;32m    263\u001B[0m \u001B[38;5;124;03m    By default, this is a wrapper around _get_text_embedding.\u001B[39;00m\n\u001B[1;32m    264\u001B[0m \u001B[38;5;124;03m    Can be overriden for batch queries.\u001B[39;00m\n\u001B[1;32m    265\u001B[0m \n\u001B[1;32m    266\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 267\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mget_embeddings\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    268\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtexts\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    269\u001B[0m \u001B[43m        \u001B[49m\u001B[43mengine\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtext_engine\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    270\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdeployment_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdeployment_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    271\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mopenai_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    272\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/Personal Projects/test_demo_for_llm/venv/lib/python3.9/site-packages/tenacity/__init__.py:289\u001B[0m, in \u001B[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001B[0;34m(*args, **kw)\u001B[0m\n\u001B[1;32m    287\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(f)\n\u001B[1;32m    288\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapped_f\u001B[39m(\u001B[38;5;241m*\u001B[39margs: t\u001B[38;5;241m.\u001B[39mAny, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw: t\u001B[38;5;241m.\u001B[39mAny) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m t\u001B[38;5;241m.\u001B[39mAny:\n\u001B[0;32m--> 289\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/Personal Projects/test_demo_for_llm/venv/lib/python3.9/site-packages/tenacity/__init__.py:379\u001B[0m, in \u001B[0;36mRetrying.__call__\u001B[0;34m(self, fn, *args, **kwargs)\u001B[0m\n\u001B[1;32m    377\u001B[0m retry_state \u001B[38;5;241m=\u001B[39m RetryCallState(retry_object\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m, fn\u001B[38;5;241m=\u001B[39mfn, args\u001B[38;5;241m=\u001B[39margs, kwargs\u001B[38;5;241m=\u001B[39mkwargs)\n\u001B[1;32m    378\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m--> 379\u001B[0m     do \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43miter\u001B[49m\u001B[43m(\u001B[49m\u001B[43mretry_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mretry_state\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    380\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(do, DoAttempt):\n\u001B[1;32m    381\u001B[0m         \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[0;32m~/Documents/Personal Projects/test_demo_for_llm/venv/lib/python3.9/site-packages/tenacity/__init__.py:326\u001B[0m, in \u001B[0;36mBaseRetrying.iter\u001B[0;34m(self, retry_state)\u001B[0m\n\u001B[1;32m    324\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreraise:\n\u001B[1;32m    325\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m retry_exc\u001B[38;5;241m.\u001B[39mreraise()\n\u001B[0;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m retry_exc \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mfut\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexception\u001B[39;00m()\n\u001B[1;32m    328\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwait:\n\u001B[1;32m    329\u001B[0m     sleep \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwait(retry_state)\n",
      "\u001B[0;31mRetryError\u001B[0m: RetryError[<Future at 0x17243faf0 state=finished raised RateLimitError>]"
     ]
    }
   ],
   "source": [
    "# UPDATE: Since llma_index changed their library, the following code should replace the code above\n",
    "index = VectorStoreIndex.from_documents(documents)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-29T18:42:42.557126Z",
     "start_time": "2023-06-29T18:42:17.671776Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# UPDATE: Since llma_index changed their library, the following code should replace the code above\n",
    "\n",
    "# Saving Index for future use. Run this cell if you need to save the index\n",
    "index.storage_context.persist()\n",
    "\n",
    "# Loading Index from local storage. Run this cell if you want to load an index to resume\n",
    "from llama_index import StorageContext, load_index_from_storage\n",
    "\n",
    "storage_context = StorageContext.from_defaults(persist_dir=\"./storage\")\n",
    "index = load_index_from_storage(storage_context)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[17], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# UPDATE: Since llma_index changed their library, the following code should replace the code above\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m query_engine \u001B[38;5;241m=\u001B[39m \u001B[43mindex\u001B[49m\u001B[38;5;241m.\u001B[39mas_query_engine()\n\u001B[1;32m      3\u001B[0m response \u001B[38;5;241m=\u001B[39m query_engine\u001B[38;5;241m.\u001B[39mquery(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExplain some of my code\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28mprint\u001B[39m(response)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'index' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# UPDATE: Since llma_index changed their library, the following code should replace the code above\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"Explain some of my code\")\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-29T18:31:25.569392Z",
     "start_time": "2023-06-29T18:31:25.555813Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
